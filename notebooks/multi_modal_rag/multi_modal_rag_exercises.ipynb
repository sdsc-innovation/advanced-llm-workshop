{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Multi-Modal RAG Hands-On Exercises\n",
    "\n",
    "In those exercises two different PDFs will be provided to the RAG pipeline: `Explainable_machine_learning_prediction_of_edema_a.pdf` and `Modeling tumor size dynamics based on real‐world electronic health records.pdf`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from helpers.data_processing import SimpleChunker, PDFExtractor\n",
    "from helpers.embedding import (\n",
    "    OpenAITextEmbeddings,\n",
    "    OpenAITextEmbeddingsAzure,\n",
    "    ImageEmbeddings,\n",
    "    ImageEmbeddingsForText,\n",
    ")\n",
    "from helpers.vectorstore import (\n",
    "    ChromaDBVectorStore,\n",
    "    VectorStoreRetriever,\n",
    ")\n",
    "from helpers.constants_and_data_classes import Roles\n",
    "from helpers.llm import OpenAILLM, OpenAILLMAzure\n",
    "from helpers.rag import Generator, DefaultRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"../../data\"\n",
    "\n",
    "pdf_files = [\n",
    "    \"Explainable_machine_learning_prediction_of_edema_a.pdf\",\n",
    "    \"Modeling tumor size dynamics based on real‐world electronic health records.pdf\",\n",
    "]\n",
    "\n",
    "text_vector_store_collection = \"text_collection\"\n",
    "image_vector_store_collection = \"image_collection\"\n",
    "\n",
    "text_vector_store_full_collection = \"text_collection_full\"\n",
    "image_vector_store_full_collection = \"image_collection_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "## If Azure Endpoint then you don't need the OPENAI_API_KEY but the following\n",
    "# os.environ[\"AZURE_API_KEY\"] = \"\"\n",
    "# os.environ[\"AZURE_API_BASE\"] = \"\"\n",
    "# os.environ[\"AZURE_API_VERSION\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_extractor = PDFExtractor()\n",
    "chunker = SimpleChunker(max_chunk_size=1000)\n",
    "\n",
    "\n",
    "text_chunks = []\n",
    "image_chunks = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(data_folder, pdf_file)\n",
    "    _, text, images = data_extractor.extract_text_and_images(pdf_path)\n",
    "    text_chunks_curr = chunker.chunk_text(text, {\"source_text\": pdf_file})\n",
    "    image_chunks_curr = chunker.chunk_images(images, {\"source_text\": pdf_file})\n",
    "    text_chunks.extend(text_chunks_curr)\n",
    "    image_chunks.extend(image_chunks_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if both Azure environment variables exist\n",
    "azure_endpoint = os.getenv(\"AZURE_API_BASE\")\n",
    "azure_api_key = os.getenv(\"AZURE_API_KEY\")\n",
    "if azure_endpoint and azure_api_key:\n",
    "    text_embedding_model = OpenAITextEmbeddingsAzure()\n",
    "    print(\"Using AzureOpenAI client\")\n",
    "else:\n",
    "    text_embedding_model = OpenAITextEmbeddings()\n",
    "    print(\"Using OpenAI client\")\n",
    "\n",
    "    \n",
    "text_embeddings = text_embedding_model.get_embedding(\n",
    "    [chunk.content for chunk in text_chunks]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_embeddings = []\n",
    "\n",
    "image_embedding_model = ImageEmbeddings()\n",
    "for chunk in tqdm(image_chunks):\n",
    "    image_embeddings.append(image_embedding_model.get_embedding(chunk.content))\n",
    "\n",
    "image_embeddings = np.array(image_embeddings)\n",
    "\n",
    "image_text_embedding_model = ImageEmbeddingsForText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store_text = ChromaDBVectorStore(text_vector_store_full_collection)\n",
    "vector_store_text.insert_chunks(text_chunks, text_embeddings)\n",
    "\n",
    "vector_store_image = ChromaDBVectorStore(image_vector_store_full_collection)\n",
    "vector_store_image.insert_chunks(image_chunks, image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = VectorStoreRetriever(\n",
    "    text_embedding_model,\n",
    "    vector_store_text,\n",
    "    image_text_embedding_model,\n",
    "    vector_store_image,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if both Azure environment variables exist\n",
    "azure_endpoint = os.getenv(\"AZURE_API_BASE\")\n",
    "azure_api_key = os.getenv(\"AZURE_API_KEY\")\n",
    "if azure_endpoint and azure_api_key:\n",
    "    llm = OpenAILLMAzure(temperature=0.3)\n",
    "    print(\"Using AzureOpenAI client\")\n",
    "else:\n",
    "    llm = OpenAILLM(temperature=0.3)\n",
    "    print(\"Using OpenAI client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"You are a helpful assistant, and your task is to answer questions using relevant chunks and images. Please first think step-by-step by mentioning which chunks you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"chunk_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\"\"\"\n",
    "print(developer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_template = \"\"\"\n",
    "Here are the relevant CHUNKS:\n",
    "{context}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Here is the USER QUESTION:\n",
    "{query}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Please think step-by-step and generate your output in json:\n",
    "\"\"\"\n",
    "print(rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = Generator(llm, developer_prompt, rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_without_images = DefaultRAG(\n",
    "    llm=llm,\n",
    "    text_embedding_model=text_embedding_model,\n",
    "    text_vector_store=vector_store_text,\n",
    "    generator=generator,\n",
    "    params={\"top_k_text\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag = DefaultRAG(\n",
    "    llm=llm,\n",
    "    text_embedding_model=text_embedding_model,\n",
    "    text_vector_store=vector_store_text,\n",
    "    image_text_embedding_model=image_text_embedding_model,\n",
    "    image_vector_store=vector_store_image,\n",
    "    generator=generator,\n",
    "    params={\"top_k_text\": 5, \"top_k_image\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, sources, cost = rag.execute(\n",
    "    \"Here goes my amazing question!\",\n",
    "    {},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The chunks retrieved by the retriever:\n",
    "print(len(sources))\n",
    "print(sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Exercises\n",
    "\n",
    "1. Explore the code\n",
    "2. Test questions and evaluate answers\n",
    "3. Discuss possible improvements\n",
    "4. (Optional - Advanced) Implemented query expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore the code\n",
    "\n",
    "Quickly go through the code and the notebooks to ensure you understand how each block works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test questions and evaluate answers\n",
    "\n",
    "The second exercise consist of testing questions and evaluating the answers. To do so, use the `rag` and `rag_without_images` pipelines defined previously and use them as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Question about text (1/2)\n",
    "\n",
    "Ask a question about `Explainable_machine_learning_prediction_of_edema_a.pdf` that can be answered with text. Use `rag_without_images`. \n",
    "\n",
    "Check the answer and verify that the chunks used belong to he correct document.\n",
    "\n",
    "If you don't have any idea, you can ask \"How did cumulative tepotinib dose impact edema predictions, and what insights did SHAP provide about this relationship?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, sources, cost = rag_without_images.execute(\n",
    "    \"Here goes my amazing question!\",\n",
    "    {},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sources))\n",
    "for source in sources:\n",
    "    print(source[\"chunk\"].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Question about text (2/2)\n",
    "\n",
    "Ask a question about `Modeling tumor size dynamics based on real‐world electronic health records.pdf` that can be answered with text. Use `rag_without_images`. \n",
    "\n",
    "Check the answer and verify that the chunks used belong to this document.\n",
    "\n",
    "If you don't have any idea, you can use \"What was the rationale for using an ON/OFF treatment effect model instead of a dose-dependent model?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, sources, cost = rag_without_images.execute(\n",
    "    \"Here goes my amazing question about the second PDF!\",\n",
    "    {},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sources))\n",
    "for source in sources:\n",
    "    print(source[\"chunk\"].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Question about a plot\n",
    "\n",
    "Find a question about a plot in one of the two documents that can not be answered using the text. \n",
    "\n",
    "First, ask the question to the only-text RAG pipeline (`rag_without_images`) and verify it can not answer it.\n",
    "\n",
    "Second, ask it to the multi-modal RAG pipeline (`rag`) and check the answer. Verify that the chunks used belong to this document.\n",
    "\n",
    "If you don't know which question to ask, you can try: \"What is the lowest SHAP value observed for 'weight' on probability of severe edema?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, sources, cost = rag_without_images.execute(\n",
    "    \"Here goes my amazing question about a plot!\",\n",
    "    {},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, sources, cost = rag.execute(\n",
    "    \"Here goes my amazing question about a plot!\",\n",
    "    {},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sources))\n",
    "for source in sources:\n",
    "    print(source[\"chunk\"].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discuss possible improvements\n",
    "\n",
    "Discuss how the pipeline could be improved to achieve better answers and identify the current pain-points. How will it be different if using a different architecture of multi-modal RAG? \n",
    "\n",
    "If time permits, try to change some parameters of the pipeline to see how it impacts the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (Optional - Advanced) Implement query expansion\n",
    "\n",
    "Implement query expansion by defining the prompt for the LLM to generate alternative queries to search more broadly in the vector store.\n",
    "\n",
    "You should provide a developer prompt, explaining to the LLM it's role (it has to find rephrasing of the query).\n",
    "\n",
    "And you should write a template for the query, stating it to provide the alternative queries based on the user query. In the template you can provide `{query}` to give it the user query and `{expansion_number}` for the number of alternative queries.\n",
    "\n",
    "The LLM should write each query on a new line.\n",
    "\n",
    "Try the results of one of the previous question, how does it impact the performance? And how does it impact the cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_expansion_developer_message = {\n",
    "    \"role\": Roles.DEVELOPER,\n",
    "    \"content\": \"Explain the role here\",\n",
    "}\n",
    "\n",
    "query_expansion_template_query = \"\"\"\n",
    "        Write the template here, use {query} and {expansion_number}\n",
    "        As a reminder each expanded query should be on its own line\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_with_query_expansion = DefaultRAG(\n",
    "    llm=llm,\n",
    "    text_embedding_model=text_embedding_model,\n",
    "    text_vector_store=vector_store_text,\n",
    "    image_text_embedding_model=image_text_embedding_model,\n",
    "    image_vector_store=vector_store_image,\n",
    "    generator=generator,\n",
    "    params={\"top_k_text\": 5, \"top_k_image\": 1, \"number_query_expansion\": 3},\n",
    "    query_expansion_developer_message=query_expansion_developer_message,\n",
    "    query_expansion_template_query=query_expansion_template_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer, sources, cost = rag_with_query_expansion.execute(\n",
    "    \"Here goes my amazing question about!\",\n",
    "    {},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
