{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Multi-Modal RAG\n",
    "\n",
    "Before reading this notebook, please make sure to have read the first document `text_rag.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### What is Multi-modal RAG?\n",
    "\n",
    "Multi-modal Retrieval-Augmented Generation (RAG) extends the standard RAG approach by incorporating **multiple types of data**—such as text, images, or even audio—**into the retrieval and generation process**. Instead of working with a single modality (like just text), multi-modal RAG systems can query and generate content based on various forms of input, allowing for richer and more diverse responses. \n",
    "\n",
    "For example, when dealing with a document that contains both images and text, a multi-modal RAG system can retrieve relevant images along with the associated text, enhancing the quality and relevance of the generated response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![Multi-Modal RAG Image](../data/multi_modal_rag.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Approaches for Working with Text and Images\n",
    "\n",
    "Multi-modal RAG (Retrieval-Augmented Generation) systems differ in how they handle text and images, depending on whether both the database and the language model (LLM) work with text and images together or focus on one modality. There are a lot of different approaches to reach this goal.\n",
    "\n",
    "Another approach not shown above is to consider the file as a series of images:\n",
    "- **File-as-Images** → **Image DB** → **Retrieve Images** → **Multi-modal LLM** → **Text answer and sources**  \n",
    "- In this case, the document is converted into a series of images (e.g., scanned pages), stored in an image-specific database, and sent as images to a multi-modal LLM, which generates textual answers based on the content of the images. The DB used is usually specialized for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In this exercise, you will learn how to implement a **Multimodal Retrieval-Augmented Generation (RAG)** pipeline from scratch, without relying on tools like `langchain`. Here, two different vector store are used to **store the images and text separately**.\n",
    "\n",
    "The different components of the pipeline are:\n",
    "\n",
    "- **Text and image extraction from PDFs** – Extract raw text and images from PDF files to make the content processable.  \n",
    "- **Text and image chunking** – Break the extracted text and images into smaller, meaningful segments to improve retrieval efficiency.  \n",
    "- **Embedding of the chunks (text and images)** – Convert text and image chunks into numerical representations (embeddings) using pre-trained models.  \n",
    "- **Storage of the embeddings in a vector store** – Save both text and image embeddings in a specialized database (vector store) to enable fast similarity searches.  \n",
    "- **Relevant chunks retrieval** – Query the vector store to find the most relevant text and image chunks based on user input.  \n",
    "- **Setting and prompting of the LLM for a RAG** – Structure prompts and configure the language model to integrate retrieved text and image information into its responses.  \n",
    "- **Additional tools for improved retrieval** – Use techniques like query expansion to reformulate user queries for better recall and reciprocal rank fusion to combine results from multiple retrieval methods.  \n",
    "- **Final multimodal RAG pipeline implementation** – Integrate all components into a complete system that retrieves relevant information (both text and images) and generates enhanced responses using the language model.\n",
    "\n",
    "**Note:** To complete this exercise, you need an OpenAI API key, the PDF files with images, and the necessary libraries installed (see `requirements.txt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import getpass\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from src.data_classes import Chunk, DataType, Roles\n",
    "from src.data_processing import PDFExtractorAPI, SimpleChunker\n",
    "from src.embedding import (\n",
    "    OpenAITextEmbeddings,\n",
    "    VLM2VecImageEmbeddings,\n",
    "    VLM2VecTextEmbeddings,\n",
    "    compute_openai_large_embedding_cost,\n",
    ")\n",
    "from src.vectorstore import (\n",
    "    ChromaDBVectorStore,\n",
    "    VectorStoreRetriever,\n",
    ")\n",
    "from src.llm import OpenAILLM\n",
    "from src.rag import Generator, DefaultRAG, query_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"../data\"\n",
    "\n",
    "pdf_files = [\n",
    "    \"Explainable_machine_learning_prediction_of_edema_a.pdf\",\n",
    "    \"Modeling tumor size dynamics based on real‐world electronic health records.pdf\",\n",
    "]\n",
    "example_pdf_file = \"Explainable_machine_learning_prediction_of_edema_a.pdf\"\n",
    "example_pdf_path = os.path.join(data_folder, example_pdf_file)\n",
    "\n",
    "text_vector_store_collection = \"text_collection\"\n",
    "image_vector_store_collection = \"image_collection\"\n",
    "\n",
    "text_vector_store_full_collection = \"text_collection_full\"\n",
    "image_vector_store_full_collection = \"image_collection_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Example\n",
    "\n",
    "The example uses only `Explainable_machine_learning_prediction_of_edema_a.pdf`. Please, have a quick look at it before starting the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_question = \"According to SHAP analysis, which factors were the most influential in predicting higher-grade edema (Grade 2+)?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## PDF Text and Images Extraction  \n",
    "\n",
    "The first step in the pipeline is to extract text and images from the document.  \n",
    "\n",
    "In this exercise, we use the `MinerU` library, which under the hood uses among others `doclayout_yolo` for segmentation. Note that this model is not commercially permissive.\n",
    "\n",
    "Extracting images can be challenging, as **irrelevant images** (such as logos) are often included, and some images may be **split into multiple images**. It may also be helpful to link the position of images to nearby text for more accurate retrieval. Specialized tools or methods might be required to efficiently handle images embedded in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_extractor = PDFExtractorAPI()\n",
    "_, text, images = data_extractor.extract_text_and_images(example_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_data = base64.b64decode(images[2][\"image_base64\"])\n",
    "img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_data = base64.b64decode(images[0][\"image_base64\"])\n",
    "img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Chunking\n",
    "\n",
    "The second step is to split the extracted text into smaller chunks, which will later be embedded and retrieved efficiently. \n",
    "\n",
    "In this exercise, we use a simple heuristic approach: the text is split iteratively—first by heading levels (`#`), then by line breaks (`\\n`), and finally by sentence (`.`). Splitting only occurs if the resulting chunk exceeds a predefined length.\n",
    "\n",
    "**Images are treated as separate chunks**, but with a different `DataType`. Additional relevant metadata can also be included, such as the image's position relative to the text or its caption, if available. They are stored in another list.\n",
    "\n",
    "Each chunk is enriched with metadata, including:  \n",
    "- **Source file** – The document from which the chunk originates.  \n",
    "- **Chunk counter** – The position of the chunk within the file.  \n",
    "- **Unique identifier (`chunk_id`)** – Ensures each chunk can be referenced independently.  \n",
    "- **Data type** - The document type (image or text).\n",
    "\n",
    "```python\n",
    "class DataType(str, Enum):\n",
    "    TEXT = \"text\"\n",
    "    IMAGE = \"image\"\n",
    "\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    chunk_id: int\n",
    "    content: str\n",
    "    metadata: dict = Field(default_factory=dict)\n",
    "    data_type: Optional[DataType] = None\n",
    "    score: Optional[float] = None\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunker = SimpleChunker()\n",
    "text_chunks = chunker.chunk_text(text, {\"source_text\": example_pdf_file})\n",
    "image_chunks = chunker.chunk_images(images, {\"source_text\": example_pdf_file})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(text_chunks))\n",
    "text_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(image_chunks))\n",
    "\n",
    "img_data = base64.b64decode(image_chunks[2].content)\n",
    "img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Embedding Models  \n",
    "\n",
    "Once the text and images are divided into chunks, each chunk is converted into a numerical representation (embedding) that captures its meaning.  \n",
    "\n",
    "For text, we use OpenAI’s `text-embedding-3-large`.\n",
    "\n",
    "For images, we utilize `VLM2Vec`. Similar to text embeddings, various options exist for image embeddings, each with its own trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = compute_openai_large_embedding_cost(text_chunks, verbose=True)\n",
    "\n",
    "text_embedding_model = OpenAITextEmbeddings()\n",
    "text_embeddings = text_embedding_model.get_embedding(\n",
    "    [chunk.content for chunk in text_chunks]\n",
    ")\n",
    "\n",
    "print(text_embeddings.shape)\n",
    "text_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_embeddings = []\n",
    "\n",
    "image_embedding_model = VLM2VecImageEmbeddings()\n",
    "for chunk in tqdm(image_chunks):\n",
    "    image_embeddings.append(image_embedding_model.get_embedding(chunk.content))\n",
    "\n",
    "\n",
    "image_embeddings = np.array(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Also define the text embedding for the image-text embedding model\n",
    "image_text_embedding_model = VLM2VecTextEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Vector Store and Retrieval  \n",
    "\n",
    "Once the chunks are embedded, they must be stored in a way that allows efficient retrieval. In this exercise, we use `ChromaDB`.  \n",
    "\n",
    "Text and image embeddings are stored separately, requiring a distinct `top_k` value for each during retrieval. Since the models used for text and image embeddings differ, their similarities cannot be directly compared. Additionally, while sparse search is not available for images, metadata filtering can still be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store_text = ChromaDBVectorStore(text_vector_store_collection)\n",
    "vector_store_text.insert_documents(text_chunks, text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store_image = ChromaDBVectorStore(image_vector_store_collection)\n",
    "vector_store_image.insert_documents(image_chunks, image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = VectorStoreRetriever(\n",
    "    text_embedding_model,\n",
    "    vector_store_text,\n",
    "    image_text_embedding_model,\n",
    "    vector_store_image,\n",
    ")\n",
    "\n",
    "results = retriever.retrieve(test_question, top_k_text=10, top_k_image=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for result_l in results:\n",
    "    for result in result_l:\n",
    "        if result[\"chunk\"].data_type == DataType.TEXT:\n",
    "            print(result)\n",
    "        elif result[\"chunk\"].data_type == DataType.IMAGE:\n",
    "            print(f\"Chunk ID: {result['chunk_id']} | Score: {result['score']}\")\n",
    "            img_data = base64.b64decode(result[\"chunk\"].content)\n",
    "            img = Image.open(io.BytesIO(img_data))\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## LLM  \n",
    "\n",
    "The LLM is the core of the RAG system, responsible for generating responses based on the retrieved information. In this case a **multi-modal LLM is required**, we use `gpt-4o-mini`.  \n",
    "\n",
    "This LLM expects input in the form of a list of messages, where each message includes the content and the role of the speaker (e.g., system, user, assistant).  \n",
    "\n",
    "Images can be provided to this LLM as `base64`, but only when the role is set to `user`.\n",
    "\n",
    "Here is how messages are defined here:\n",
    "\n",
    "```python\n",
    "class Roles(str, Enum):\n",
    "    SYSTEM = \"system\"\n",
    "    USER = \"user\"\n",
    "    ASSISTANT = \"assistant\"\n",
    "    TOOL = \"tool\"\n",
    "\n",
    "class LLMMessage(BaseModel):\n",
    "    content: Optional[str] = None\n",
    "    role: Optional[Roles] = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAILLM(temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_data = base64.b64decode(image_chunks[2].content)\n",
    "img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, cost = llm.generate(\n",
    "    [\n",
    "        {\n",
    "            \"role\": Roles.USER,\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": test_question},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_chunks[2].content}\"\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Generator  \n",
    "\n",
    "Once the LLM is set up, a specific prompt needs to be defined for the RAG system. This prompt must include the retrieved chunks as context. The prompt has to be adapted to each specific project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_system_prompt = \"\"\"You are a helpful assistant, and your task is to answer questions using relevant documents and images. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\"\"\"\n",
    "print(default_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_rag_template = \"\"\"\n",
    "Here are the relevant DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Here is the USER QUESTION:\n",
    "{query}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Please think step-by-step and generate your output in json:\n",
    "\"\"\"\n",
    "print(default_rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = Generator(llm, default_system_prompt, default_rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, cost = generator.generate(\n",
    "    history=[],\n",
    "    query=test_question,\n",
    "    chunks=[\n",
    "        Chunk(\n",
    "            chunk_id=0,\n",
    "            data_type=DataType.IMAGE,\n",
    "            content=image_chunks[2].content,\n",
    "            metadata={},\n",
    "        ),\n",
    "        Chunk(\n",
    "            chunk_id=1,\n",
    "            data_type=DataType.TEXT,\n",
    "            content=text_chunks[0].content,\n",
    "            metadata={},\n",
    "        ),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## RAG Tools  \n",
    "\n",
    "There are several methods to improve the efficiency of a RAG pipeline.\n",
    "\n",
    "In this notebook, we implement **query expansion** to enhance retrieval and apply **reciprocal rank fusion** to optimize the ranking of chunks when multiple queries are involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_expansion_system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a focused assistant designed to generate multiple, relevant search queries based solely on a single input query. Your task is to produce a list of these queries in English, without adding any further explanations or information.\",\n",
    "}\n",
    "\n",
    "query_expansion_template_query = \"\"\"\n",
    "        Generate multiple search queries related to: {query}, and translate them in english if they are not already in english. Only output {expansion_number} queries in english.\n",
    "        OUTPUT ({expansion_number} queries):\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, cost = query_expansion(\n",
    "    test_question,\n",
    "    llm,\n",
    "    query_expansion_system_message,\n",
    "    query_expansion_template_query,\n",
    "    expansion_number=5,\n",
    ")\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## RAG  \n",
    "\n",
    "Finally, the RAG pipeline is defined by integrating all the previously discussed components into a unified process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag = DefaultRAG(\n",
    "    llm,\n",
    "    text_embedding_model,\n",
    "    vector_store_text,\n",
    "    generator,\n",
    "    query_expansion_system_message,\n",
    "    query_expansion_template_query,\n",
    "    {\"top_k_text\": 5, \"top_k_image\": 3, \"number_query_expansion\": 0},\n",
    "    image_text_embedding_model,\n",
    "    vector_store_image,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, sources, cost = rag.execute(test_question, {}, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The documents retrieved by the retriever:\n",
    "print(len(sources))\n",
    "print(sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Exercises\n",
    "\n",
    "The different blocks are redefined below, and a new pipeline is created that uses both PDFs.\n",
    "\n",
    "1. Quickly go through the code and the above notebook to ensure you understand how each block works, focus on the how the images are handled in the pipeline.\n",
    "2. Try to formulate a question about another plot in `Explainable_machine_learning_prediction_of_edema_a.pdf` that could only be explained using it, and not the text. Analyze the answer and verify it uses the image, try the same when not providing the images to the RAG.\n",
    "3. Do the same for `Modeling tumor size dynamics based on real‐world electronic health records.pdf`, verify that the images retrieved indeed belong to it.\n",
    "4. Discuss how the pipeline could be improved to achieve better answers and identify the current pain-points. How will it be different if using a different architecture of multi-modal RAG? If time permits, implement those changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_extractor = PDFExtractorAPI()\n",
    "chunker = SimpleChunker(max_chunk_size=1000)\n",
    "\n",
    "\n",
    "text_chunks = []\n",
    "image_chunks = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    print(pdf_file)\n",
    "    pdf_path = os.path.join(data_folder, pdf_file)\n",
    "    _, text, images = data_extractor.extract_text_and_images(pdf_path)\n",
    "    text_chunks_curr = chunker.chunk_text(text, {\"source_text\": pdf_file})\n",
    "    image_chunks_curr = chunker.chunk_images(images, {\"source_text\": pdf_file})\n",
    "    text_chunks.extend(text_chunks_curr)\n",
    "    image_chunks.extend(image_chunks_curr)\n",
    "\n",
    "print(len(text_chunks))\n",
    "print(len(image_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_embedding_model = OpenAITextEmbeddings()\n",
    "text_embeddings = text_embedding_model.get_embedding(\n",
    "    [chunk.content for chunk in text_chunks]\n",
    ")\n",
    "print(text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_embeddings = []\n",
    "\n",
    "image_embedding_model = VLM2VecImageEmbeddings()\n",
    "for chunk in tqdm(image_chunks):\n",
    "    image_embeddings.append(image_embedding_model.get_embedding(chunk.content))\n",
    "\n",
    "image_embeddings = np.array(image_embeddings)\n",
    "\n",
    "image_text_embedding_model = VLM2VecTextEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store_text = ChromaDBVectorStore(text_vector_store_full_collection)\n",
    "vector_store_text.insert_documents(text_chunks, text_embeddings)\n",
    "\n",
    "vector_store_image = ChromaDBVectorStore(image_vector_store_full_collection)\n",
    "vector_store_image.insert_documents(image_chunks, image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = VectorStoreRetriever(\n",
    "    text_embedding_model,\n",
    "    vector_store_text,\n",
    "    image_text_embedding_model,\n",
    "    vector_store_image,\n",
    ")\n",
    "\n",
    "results = retriever.retrieve(test_question, top_k_text=10, top_k_image=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAILLM(temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant, and your task is to answer questions using relevant documents and images. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\"\"\"\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_template = \"\"\"\n",
    "Here are the relevant DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Here is the USER QUESTION:\n",
    "{query}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Please think step-by-step and generate your output in json:\n",
    "\"\"\"\n",
    "print(rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_expansion_system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a focused assistant designed to generate multiple, relevant search queries based solely on a single input query. Your task is to produce a list of these queries in English, without adding any further explanations or information.\",\n",
    "}\n",
    "\n",
    "query_expansion_template_query = \"\"\"\n",
    "        Generate multiple search queries related to: {query}, and translate them in english if they are not already in english. Only output {expansion_number} queries in english.\n",
    "        OUTPUT ({expansion_number} queries):\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = Generator(llm, system_prompt, rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag = DefaultRAG(\n",
    "    llm,\n",
    "    text_embedding_model,\n",
    "    vector_store_text,\n",
    "    generator,\n",
    "    query_expansion_system_message,\n",
    "    query_expansion_template_query,\n",
    "    {\"top_k_text\": 5, \"top_k_image\": 3, \"number_query_expansion\": 0},\n",
    "    image_text_embedding_model,\n",
    "    vector_store_image,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, sources, cost = rag.execute(\n",
    "    \"Here goes my amazing question!\",\n",
    "    {},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The documents retrieved by the retriever:\n",
    "print(len(sources))\n",
    "print(sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-llm-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
