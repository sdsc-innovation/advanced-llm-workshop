{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce52a519fcb861f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# LLM and data extraction\n",
    "\n",
    "In this notebook, we will explore how to use the OpenAI API to extract metadata from scientific papers. We will use a PDF file as input and convert it to markdown text. Then, we will use the OpenAI API to extract the title, authors, and abstract from the markdown text.\n",
    "\n",
    "We will compare different methods to extract metadata from scientific papers using the OpenAI API, including:\n",
    "- Asking the API to extract the metadata directly from the markdown text.\n",
    "- Asking the API to extract the metadata and return the result in JSON format.\n",
    "- Using a JSON schema to define the expected output format.\n",
    "- Using Pydantic models to define the expected output format.\n",
    "- Using function calls to extract metadata from the markdown text.\n",
    "\n",
    "In all the following exemple we'll extract the same information on all these articles:\n",
    "- Title\n",
    "- Authors\n",
    "- Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696f294-70e8-42e3-a5aa-a70ebb3fc22b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Why JSON?\n",
    "\n",
    "- **Interoperability**: JSON is language-agnostic and easily parsed in Python, R, and other languages.\n",
    "- **API Integration**: Many data sources and web services provide data in JSON format, making it essential for fetching and processing external data.\n",
    "- **Hierarchical Structure**: Supports nested data, making it ideal for representing complex datasets like configurations or structured logs.\n",
    "- **Integration with Pandas**: Python's `pandas` library provides seamless methods (`pd.read_json`, `to_json`) for handling JSON data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c27ff0b356d5754",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Initialize the OpenAI client and load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8436326871e6351",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7859f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==2.2.2 (from -r ../../requirements.txt (line 1))\n",
      "  Downloading numpy-2.2.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas==2.2.3 (from -r ../../requirements.txt (line 2))\n",
      "  Downloading pandas-2.2.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting pydantic==2.11.4 (from -r ../../requirements.txt (line 3))\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting openai==1.75.0 (from -r ../../requirements.txt (line 4))\n",
      "  Downloading openai-1.75.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting chromadb==1.0.9 (from -r ../../requirements.txt (line 5))\n",
      "  Downloading chromadb-1.0.9-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting matplotlib==3.10.0 (from -r ../../requirements.txt (line 6))\n",
      "  Downloading matplotlib-3.10.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting tiktoken==0.9.0 (from -r ../../requirements.txt (line 7))\n",
      "  Downloading tiktoken-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting smolagents==1.15.0 (from smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading smolagents-1.15.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting markdownify==1.1.0 (from -r ../../requirements.txt (line 10))\n",
      "  Downloading markdownify-1.1.0-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting duckduckgo_search==8.0.1 (from -r ../../requirements.txt (line 11))\n",
      "  Downloading duckduckgo_search-8.0.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting requests==2.32.3 (from -r ../../requirements.txt (line 12))\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pymupdf4llm==0.0.22 (from -r ../../requirements.txt (line 13))\n",
      "  Downloading pymupdf4llm-0.0.22-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting ipykernel==6.29.5 (from -r ../../requirements.txt (line 14))\n",
      "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from pandas==2.2.3->-r ../../requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from pandas==2.2.3->-r ../../requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from pandas==2.2.3->-r ../../requirements.txt (line 2)) (2025.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic==2.11.4->-r ../../requirements.txt (line 3))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic==2.11.4->-r ../../requirements.txt (line 3))\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from pydantic==2.11.4->-r ../../requirements.txt (line 3)) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic==2.11.4->-r ../../requirements.txt (line 3))\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai==1.75.0->-r ../../requirements.txt (line 4))\n",
      "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai==1.75.0->-r ../../requirements.txt (line 4))\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai==1.75.0->-r ../../requirements.txt (line 4))\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai==1.75.0->-r ../../requirements.txt (line 4))\n",
      "  Downloading jiter-0.10.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai==1.75.0->-r ../../requirements.txt (line 4))\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from openai==1.75.0->-r ../../requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: build>=1.0.3 in /layers/paketo-buildpacks_poetry/poetry/lib/python3.13/site-packages (from chromadb==1.0.9->-r ../../requirements.txt (line 5)) (1.3.0)\n",
      "Collecting fastapi==0.115.9 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading posthog-6.7.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading onnxruntime-1.22.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.57b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from chromadb==1.0.9->-r ../../requirements.txt (line 5)) (0.21.4)\n",
      "Collecting pypika>=0.48.9 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting overrides>=7.3.1 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading grpcio-1.74.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading typer-0.16.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from chromadb==1.0.9->-r ../../requirements.txt (line 5)) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading mmh3-5.2.0-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading orjson-3.11.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting rich>=10.11.0 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting jsonschema>=4.19.0 (from chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib==3.10.0->-r ../../requirements.txt (line 6)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib==3.10.0->-r ../../requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib==3.10.0->-r ../../requirements.txt (line 6)) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib==3.10.0->-r ../../requirements.txt (line 6)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib==3.10.0->-r ../../requirements.txt (line 6)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib==3.10.0->-r ../../requirements.txt (line 6)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib==3.10.0->-r ../../requirements.txt (line 6)) (3.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from tiktoken==0.9.0->-r ../../requirements.txt (line 7)) (2025.7.34)\n",
      "Requirement already satisfied: huggingface-hub>=0.30.0 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from smolagents==1.15.0->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8)) (0.34.3)\n",
      "Collecting jinja2>=3.1.4 (from smolagents==1.15.0->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting python-dotenv (from smolagents==1.15.0->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting beautifulsoup4<5,>=4.9 (from markdownify==1.1.0->-r ../../requirements.txt (line 10))\n",
      "  Downloading beautifulsoup4-4.13.5-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: six<2,>=1.15 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from markdownify==1.1.0->-r ../../requirements.txt (line 10)) (1.17.0)\n",
      "Collecting click>=8.1.8 (from duckduckgo_search==8.0.1->-r ../../requirements.txt (line 11))\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting primp>=0.15.0 (from duckduckgo_search==8.0.1->-r ../../requirements.txt (line 11))\n",
      "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting lxml>=5.3.0 (from duckduckgo_search==8.0.1->-r ../../requirements.txt (line 11))\n",
      "  Downloading lxml-6.0.1-cp313-cp313-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from requests==2.32.3->-r ../../requirements.txt (line 12)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from requests==2.32.3->-r ../../requirements.txt (line 12)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from requests==2.32.3->-r ../../requirements.txt (line 12)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from requests==2.32.3->-r ../../requirements.txt (line 12)) (2025.8.3)\n",
      "Collecting pymupdf>=1.25.5 (from pymupdf4llm==0.0.22->-r ../../requirements.txt (line 13))\n",
      "  Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.1 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (9.4.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (1.6.0)\n",
      "Requirement already satisfied: psutil in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (27.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (5.14.3)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting gradio>=5.13.2 (from smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading gradio-5.44.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting litellm>=1.60.2 (from smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading litellm-1.76.0-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5,>=4.9->markdownify==1.1.0->-r ../../requirements.txt (line 10))\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.75.0->-r ../../requirements.txt (line 4))\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.75.0->-r ../../requirements.txt (line 4))\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: pyproject_hooks in /layers/paketo-buildpacks_poetry/poetry/lib/python3.13/site-packages (from build>=1.0.3->chromadb==1.0.9->-r ../../requirements.txt (line 5)) (1.2.0)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting audioop-lts<1.0 (from gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading audioop_lts-0.2.2-cp313-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting brotli>=1.1.0 (from gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading Brotli-1.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting ffmpy (from gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading ffmpy-0.6.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.12.1 (from gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading gradio_client-1.12.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting markupsafe<4.0,>=2.0 (from gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting pydub (from gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading ruff-0.12.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /layers/paketo-buildpacks_poetry/poetry/lib/python3.13/site-packages (from gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9)) (0.13.3)\n",
      "Requirement already satisfied: fsspec in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from gradio-client==1.12.1->gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9)) (2025.7.0)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.12.1->gradio>=5.13.2->smolagents[gradio]==1.15.0->-r ../../requirements.txt (line 9))\n",
      "  Downloading websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: filelock in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from huggingface-hub>=0.30.0->smolagents==1.15.0->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8)) (3.18.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from huggingface-hub>=0.30.0->smolagents==1.15.0->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8)) (1.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /layers/paketo-buildpacks_poetry/poetry/lib/python3.13/site-packages (from typer>=0.9.0->chromadb==1.0.9->-r ../../requirements.txt (line 5)) (1.5.4)\n",
      "Requirement already satisfied: decorator in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /layers/paketo-buildpacks_poetry/poetry/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (0.8.4)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.19.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.19.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.19.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.19.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading rpds_py-0.27.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /layers/paketo-buildpacks_poetry/poetry/lib/python3.13/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (4.3.8)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting aiohttp>=3.10 (from litellm>=1.60.2->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading aiohttp-3.12.15-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting importlib-metadata>=6.8.0 (from litellm>=1.60.2->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "INFO: pip is looking at multiple versions of litellm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting litellm>=1.60.2 (from smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading litellm-1.75.9-py3-none-any.whl.metadata (41 kB)\n",
      "  Downloading litellm-1.75.8-py3-none-any.whl.metadata (41 kB)\n",
      "  Downloading litellm-1.75.7-py3-none-any.whl.metadata (41 kB)\n",
      "  Downloading litellm-1.75.6-py3-none-any.whl.metadata (41 kB)\n",
      "  Downloading litellm-1.75.5.post2-py3-none-any.whl.metadata (41 kB)\n",
      "  Downloading litellm-1.75.5.post1-py3-none-any.whl.metadata (41 kB)\n",
      "  Downloading litellm-1.75.4-py3-none-any.whl.metadata (40 kB)\n",
      "INFO: pip is still looking at multiple versions of litellm to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading litellm-1.75.3-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.10->litellm>=1.60.2->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp>=3.10->litellm>=1.60.2->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.10->litellm>=1.60.2->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading frozenlist-1.7.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.10->litellm>=1.60.2->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading multidict-6.6.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp>=3.10->litellm>=1.60.2->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading propcache-0.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp>=3.10->litellm>=1.60.2->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading yarl-1.20.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=6.8.0->litellm>=1.60.2->smolagents[litellm]==1.15.0->-r ../../requirements.txt (line 8))\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk>=1.2.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.57b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.57b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.57b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading opentelemetry_instrumentation-0.57b0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting opentelemetry-util-http==0.57b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading opentelemetry_util_http-0.57b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.57b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading wrapt-1.17.3-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.57b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading asgiref-3.9.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /layers/paketo-buildpacks_poetry/poetry/lib/python3.13/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (0.7.0)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading httptools-0.6.4-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading uvloop-0.21.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading watchfiles-1.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from stack_data->ipython>=7.23.1->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from stack_data->ipython>=7.23.1->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages (from stack_data->ipython>=7.23.1->ipykernel==6.29.5->-r ../../requirements.txt (line 14)) (0.2.3)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb==1.0.9->-r ../../requirements.txt (line 5))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading numpy-2.2.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m130.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading openai-1.75.0-py3-none-any.whl (646 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.0/647.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chromadb-1.0.9-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smolagents-1.15.0-py3-none-any.whl (124 kB)\n",
      "Downloading markdownify-1.1.0-py3-none-any.whl (13 kB)\n",
      "Downloading duckduckgo_search-8.0.1-py3-none-any.whl (18 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading pymupdf4llm-0.0.22-py3-none-any.whl (28 kB)\n",
      "Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
      "Downloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Downloading beautifulsoup4-4.13.5-py3-none-any.whl (105 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.10.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (350 kB)\n",
      "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading gradio-5.44.0-py3-none-any.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading gradio_client-1.12.1-py3-none-any.whl (324 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading audioop_lts-0.2.2-cp313-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (85 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading orjson-3.11.3-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Downloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading typer-0.16.1-py3-none-any.whl (46 kB)\n",
      "Downloading websockets-15.0.1-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Downloading Brotli-1.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.74.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading litellm-1.75.3-py3-none-any.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.15-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.6.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (254 kB)\n",
      "Downloading yarl-1.20.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading lxml-6.0.1-cp313-cp313-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.2.0-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading onnxruntime-1.22.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.57b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.57b0-py3-none-any.whl (32 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.57b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_util_http-0.57b0-py3-none-any.whl (7.6 kB)\n",
      "Downloading asgiref-3.9.1-py3-none-any.whl (23 kB)\n",
      "Downloading wrapt-1.17.3-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-6.7.0-py3-none-any.whl (122 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading pymupdf-1.26.4-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading rpds_py-0.27.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (386 kB)\n",
      "Downloading ruff-0.12.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m130.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading httptools-0.6.4-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (473 kB)\n",
      "Downloading uvloop-0.21.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading ffmpy-0.6.1-py3-none-any.whl (5.5 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=9f230a34a5a95b45ba50fb5065371e58c8948344b90791a71ccfb331be0dc82e\n",
      "  Stored in directory: /home/renku/.cache/pip/wheels/b4/f8/a5/28e9c1524d320f4b8eefdce0e487b5c2e128dbf2ed1bb4a60b\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, pydub, mpmath, flatbuffers, durationpy, brotli, zipp, wrapt, websockets, websocket-client, uvloop, typing-inspection, tenacity, sympy, soupsieve, sniffio, semantic-version, ruff, rpds-py, requests, python-multipart, python-dotenv, pymupdf, pydantic-core, pyasn1, protobuf, propcache, primp, overrides, orjson, opentelemetry-util-http, oauthlib, numpy, multidict, mmh3, mdurl, markupsafe, lxml, jiter, importlib-resources, humanfriendly, httptools, h11, grpcio, groovy, frozenlist, ffmpy, distro, click, cachetools, bcrypt, backoff, audioop-lts, attrs, asgiref, annotated-types, aiohappyeyeballs, aiofiles, yarl, uvicorn, tiktoken, rsa, requests-oauthlib, referencing, pymupdf4llm, pydantic, pyasn1-modules, posthog, pandas, opentelemetry-proto, markdown-it-py, jinja2, importlib-metadata, httpcore, googleapis-common-protos, duckduckgo_search, coloredlogs, beautifulsoup4, anyio, aiosignal, watchfiles, starlette, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, matplotlib, markdownify, jsonschema-specifications, ipykernel, httpx, google-auth, aiohttp, typer, smolagents, safehttpx, opentelemetry-semantic-conventions, openai, kubernetes, jsonschema, gradio-client, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, litellm, gradio, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "\u001b[2K  Attempting uninstall: requestsm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 17/110\u001b[0m [ruff]io]ts]\n",
      "\u001b[2K    Found existing installation: requests 2.32.4━━━━━━━━━━━━━━\u001b[0m \u001b[32m 17/110\u001b[0m [ruff]\n",
      "\u001b[2K    Not uninstalling requests at /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages, outside environment /home/renku/work/.venv\n",
      "\u001b[2K    Can't uninstall 'requests'. No files were found to uninstall.m \u001b[32m 17/110\u001b[0m [ruff]\n",
      "\u001b[2K  Attempting uninstall: numpy\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 31/110\u001b[0m [oauthlib]\n",
      "\u001b[2K    Found existing installation: numpy 2.3.2━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 31/110\u001b[0m [oauthlib]\n",
      "\u001b[2K    Not uninstalling numpy at /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages, outside environment /home/renku/work/.venv\n",
      "\u001b[2K    Can't uninstall 'numpy'. No files were found to uninstall.\u001b[0m \u001b[32m 31/110\u001b[0m [oauthlib]\n",
      "\u001b[2K  Attempting uninstall: pandas━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 66/110\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K    Found existing installation: pandas 2.3.10m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 66/110\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K    Not uninstalling pandas at /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages, outside environment /home/renku/work/.venv\n",
      "\u001b[2K    Can't uninstall 'pandas'. No files were found to uninstall.[0m \u001b[32m 66/110\u001b[0m [pyasn1-modules]\n",
      "\u001b[2K  Attempting uninstall: matplotlib━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m 85/110\u001b[0m [onnxruntime]y-api]\n",
      "\u001b[2K    Found existing installation: matplotlib 3.10.590m━━━━━━━━━\u001b[0m \u001b[32m 85/110\u001b[0m [onnxruntime]\n",
      "\u001b[2K    Not uninstalling matplotlib at /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages, outside environment /home/renku/work/.venv\n",
      "\u001b[2K    Can't uninstall 'matplotlib'. No files were found to uninstall.\u001b[32m 85/110\u001b[0m [onnxruntime]\n",
      "\u001b[2K  Attempting uninstall: ipykernel━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m 88/110\u001b[0m [jsonschema-specifications]\n",
      "\u001b[2K    Found existing installation: ipykernel 6.30.1m\u001b[90m━━━━━━━\u001b[0m \u001b[32m 88/110\u001b[0m [jsonschema-specifications]\n",
      "\u001b[2K    Not uninstalling ipykernel at /layers/renku_kernel-installer/kernel/lib/python3.13/site-packages, outside environment /home/renku/work/.venv\n",
      "\u001b[2K    Can't uninstall 'ipykernel'. No files were found to uninstall. \u001b[32m 88/110\u001b[0m [jsonschema-specifications]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110/110\u001b[0m [chromadb]chromadb]metry-instrumentation-asgi]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiofiles-24.1.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.10.0 asgiref-3.9.1 attrs-25.3.0 audioop-lts-0.2.2 backoff-2.2.1 bcrypt-4.3.0 beautifulsoup4-4.13.5 brotli-1.1.0 cachetools-5.5.2 chromadb-1.0.9 click-8.2.1 coloredlogs-15.0.1 distro-1.9.0 duckduckgo_search-8.0.1 durationpy-0.10 fastapi-0.115.9 ffmpy-0.6.1 flatbuffers-25.2.10 frozenlist-1.7.0 google-auth-2.40.3 googleapis-common-protos-1.70.0 gradio-5.44.0 gradio-client-1.12.1 groovy-0.1.2 grpcio-1.74.0 h11-0.16.0 httpcore-1.0.9 httptools-0.6.4 httpx-0.28.1 humanfriendly-10.0 importlib-metadata-8.7.0 importlib-resources-6.5.2 ipykernel-6.29.5 jinja2-3.1.6 jiter-0.10.0 jsonschema-4.25.1 jsonschema-specifications-2025.4.1 kubernetes-33.1.0 litellm-1.75.3 lxml-6.0.1 markdown-it-py-4.0.0 markdownify-1.1.0 markupsafe-3.0.2 matplotlib-3.10.0 mdurl-0.1.2 mmh3-5.2.0 mpmath-1.3.0 multidict-6.6.4 numpy-2.2.2 oauthlib-3.3.1 onnxruntime-1.22.1 openai-1.75.0 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-instrumentation-0.57b0 opentelemetry-instrumentation-asgi-0.57b0 opentelemetry-instrumentation-fastapi-0.57b0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 opentelemetry-util-http-0.57b0 orjson-3.11.3 overrides-7.7.0 pandas-2.2.3 posthog-6.7.0 primp-0.15.0 propcache-0.3.2 protobuf-6.32.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.4 pydantic-core-2.33.2 pydub-0.25.1 pymupdf-1.26.4 pymupdf4llm-0.0.22 pypika-0.48.9 python-dotenv-1.1.1 python-multipart-0.0.20 referencing-0.36.2 requests-2.32.3 requests-oauthlib-2.0.0 rich-14.1.0 rpds-py-0.27.1 rsa-4.9.1 ruff-0.12.10 safehttpx-0.1.6 semantic-version-2.10.0 smolagents-1.15.0 sniffio-1.3.1 soupsieve-2.7 starlette-0.45.3 sympy-1.14.0 tenacity-9.1.2 tiktoken-0.9.0 typer-0.16.1 typing-inspection-0.4.1 uvicorn-0.35.0 uvloop-0.21.0 watchfiles-1.1.0 websocket-client-1.8.0 websockets-15.0.1 wrapt-1.17.3 yarl-1.20.1 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "553e1171e3d7920",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "\n",
    "from helpers.data_processing import PDFExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3f67bcdd61fce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "## If Azure Endpoint then you don't need the OPENAI_API_KEY but the following\n",
    "# os.environ[\"AZURE_API_KEY\"] = \"\"\n",
    "# os.environ[\"AZURE_API_BASE\"] = \"\"\n",
    "# os.environ[\"AZURE_API_VERSION\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48713877fe6308ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing AzureOpenAI client\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing OpenAI client\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.13/site-packages/openai/_client.py:116\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    114\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    117\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m     )\n\u001b[32m    119\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Check if both Azure environment variables exist\n",
    "azure_endpoint = os.getenv(\"AZURE_API_BASE\")\n",
    "azure_api_key = os.getenv(\"AZURE_API_KEY\")\n",
    "azure_api_version = os.environ[\"AZURE_API_VERSION\"]\n",
    "\n",
    "if azure_endpoint and azure_api_key:\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=azure_endpoint,\n",
    "        api_key=azure_api_key,\n",
    "        api_version=azure_api_version\n",
    "    )\n",
    "    print(\"Using AzureOpenAI client\")\n",
    "else:\n",
    "    client = OpenAI()\n",
    "    print(\"Using OpenAI client\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa4dc7653db9f64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Load pdf and convert to markdown\n",
    "\n",
    "Here we're using the `pymupdf4llm` library to convert a PDF file to markdown. There are other alternatives such as `textract` and `docling` that can be used to extract text from PDF files. After the workshop feel free to try different libraries and compare the results.\n",
    "\n",
    "We downloaded 2 articles from Pubmed to showcase the process of data extraction. Feel free to try both articles and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9992e9fdc0a91f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the PDF file\n",
    "pdf_path = \"../../data/Explainable_machine_learning_prediction_of_edema_a.pdf\"\n",
    "# pdf_path = \"../../data/Modeling tumor size dynamics based on real‐world electronic health records.pdf\"\n",
    "\n",
    "# Convert the PDF file to markdown\n",
    "# markdown_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "\n",
    "data_extractor = PDFExtractor()\n",
    "_, markdown_text, _ = data_extractor.extract_text_and_images(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74d0515a77808b",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(markdown_text[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4c924c9414ed7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Default extraction\n",
    "\n",
    "In this case, we will provide a prompt asking the API to extract the title, authors, and abstract from the markdown text. No extra indications are given to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9b2b242da9ab0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_completion(message: str):\n",
    "    return client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": message}],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999270a2c5c59884",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a document processing assistant. I have extracted the following markdown text from a PDF.\n",
    "Please extract the following details:\n",
    "- Title\n",
    "- Authors\n",
    "- Abstract\n",
    "\n",
    "Markdown text:\n",
    "{markdown_text}\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a115d16021b686f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88809827a33ea91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### Result\n",
    "\n",
    "We can see here that the LLM model was able to extract the title, authors, and abstract from the markdown text. The result is returned as plain text in a markdown format. This format is not very structured and may require additional processing to extract the information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3feb7fa15cce91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Asking for JSON format\n",
    "\n",
    "Here we're adding one step more. We're asking the LLM to return the result in JSON format. This way we can have a more structured output and it will be easier to extract the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcfe80fc2bb462d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a document processing assistant. I have extracted the following markdown text from a PDF.\n",
    "Please extract the following details:\n",
    "- Title\n",
    "- Authors\n",
    "- Abstract\n",
    "\n",
    "Markdown text:\n",
    "{markdown_text}\n",
    "\n",
    "Give me the result in JSON format.\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3adde0f32c3dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b469514bfacb02",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.loads(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ff68f0d842eed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Result\n",
    "\n",
    "The result is not returned in JSON format as requested. The model still returns the information in plain text containing the JSON code. The data is structured as JSON, but it is not returned as a JSON object. We will need to process the text to extract the JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c6670ee38a3c27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove the markdown code block markers\n",
    "json_str = re.sub(r\"^```(?:json)?\\s*\", \"\", completion.choices[0].message.content)\n",
    "json_str = re.sub(r\"\\s*```$\", \"\", json_str)\n",
    "\n",
    "# Parse the JSON string\n",
    "json.loads(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0bd90c5bab3e50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Enters `response_format`\n",
    "\n",
    "OpenAI allows us to specify the response format to be \"json_object\". This way we can force the model to return the result in JSON format. That way the parsing of the result will be easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4a91849c0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_completion_json(message: str):\n",
    "    return client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": message}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50caccb7fb23d65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a document processing assistant. I have extracted the following markdown text from a PDF.\n",
    "Please extract the following details:\n",
    "- Title\n",
    "- Authors\n",
    "- Abstract\n",
    "\n",
    "Markdown text:\n",
    "{markdown_text}\n",
    "\n",
    "Give me the result in JSON format.\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_completion_json(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609710ce91bdf3f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e99ecb99b79a0a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.loads(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108d1e70e12ceb0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### Result\n",
    "\n",
    "This time the result is returned in JSON format as requested. We can directly parse the JSON object to extract the information using `json.loads()`. However, there is no guarantee that the JSON object will have the expected structure. The model may return the data in a different format than the one we expect for exemple with different casing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db61eefdd694c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Custom json schema\n",
    "\n",
    "This time we'll pass a json schema to the model as defined here: https://json-schema.org/. This way we can force the model to return the result in a specific structure, provide default values, descriptions and types for each field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e79af00e4f478",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_completion_json_schema(message: str, schema: dict):\n",
    "    return client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": message}],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\"name\": \"ExtractedData\", \"schema\": schema},\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efab2aecdfdd6f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"name\": \"ExtractedData\",\n",
    "    \"description\": \"Metadata for a research article including its title, list of authors, and abstract summary.\",\n",
    "    \"properties\": {\n",
    "        \"title\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The title of the research article.\",\n",
    "            \"default\": \"Unknown\",\n",
    "        },\n",
    "        \"authors\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"description\": \"A list of authors who contributed to the article.\",\n",
    "        },\n",
    "        \"abstract\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A brief summary of the article's content and findings.\",\n",
    "        },\n",
    "    },\n",
    "    \"additionalProperties\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e9970-df3a-443e-94fd-1fab99d36b2d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a document processing assistant. I have extracted the following markdown text from a PDF.\n",
    "Please extract the following details:\n",
    "- Title\n",
    "- Authors\n",
    "- Abstract\n",
    "\n",
    "Markdown text:\n",
    "{markdown_text}\n",
    "\n",
    "Give me the result in JSON format.\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_completion_json_schema(prompt, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6514e766d8ea94",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.loads(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6b52d-b48e-4a37-87aa-52f4b077d4fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### Result\n",
    "\n",
    "Now the output will always correspond to the expected schema. Since everything is provided the model doesn't have to guess the shape or part of the shape of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d130902a04702",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Using the Types\n",
    "\n",
    "Alternatively, we can use Pydantic models to define the expected output format (cf. https://docs.pydantic.dev/latest/). This way we can enforce the structure of the output and provide additional type checking. This can be particularly useful when working with APIs that expect a specific format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e06a2a2e4112f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ExtractedData(BaseModel):\n",
    "    title: str = Field(..., description=\"The title of the research article.\")\n",
    "    authors: List[str] = Field(\n",
    "        ..., description=\"A list of authors who contributed to the article.\"\n",
    "    )\n",
    "    abstract: str = Field(\n",
    "        ..., description=\"A brief summary of the article's content and findings.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_completion_pydantic(message: str):\n",
    "    return client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": message}],\n",
    "        response_format=ExtractedData,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ba109ab524638",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a document processing assistant. I have extracted the following markdown text from a PDF.\n",
    "Please extract the following details:\n",
    "- Title\n",
    "- Authors\n",
    "- Abstract\n",
    "\n",
    "Markdown text:\n",
    "{markdown_text}\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_completion_pydantic(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ad6ac9c8dbe4d",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.loads(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf79a16b0ddffc6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Function calling\n",
    "\n",
    "Many LLMs don't support the structured output format. In that case you can specify to the llm to call a function to extract the information. This way you can define the function signature and the llm will call the function with the extracted information.\n",
    "\n",
    "![Tool Calling](images/tool_calling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aeb79375c8b40b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "article_extraction_function_description = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"extract_article_data\",\n",
    "        \"description\": \"Extract article metadata from markdown text.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"title\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The title of the research article.\",\n",
    "                },\n",
    "                \"authors\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                    \"description\": \"A list of authors who contributed to the article.\",\n",
    "                },\n",
    "                \"abstract\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A brief summary of the article's content and findings.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"title\", \"authors\", \"abstract\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def generate_completion_tool_calls(message: str):\n",
    "    return client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": message}],\n",
    "        tools=[article_extraction_function_description],\n",
    "        tool_choice=\"auto\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c433112fd0d0c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a document processing assistant. I have extracted the following markdown text from a PDF.\n",
    "Please use the given tools to extract the following details:\n",
    "- Title\n",
    "- Authors\n",
    "- Abstract\n",
    "\n",
    "Markdown text:\n",
    "{markdown_text}\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_completion_tool_calls(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81fe16153e3641",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.loads(completion.choices[0].message.tool_calls[0].function.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cedb74-5c8d-47e2-be6f-c60e640adf92",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Notes\n",
    "\n",
    "Here you can note that we don't call the functions. The goal is not to use them as a tool call, but as a way of extracting the data from the documents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995dcb724cfc267",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Multiple function calls\n",
    "\n",
    "It's important to note that you can ask the llm to call multiple functions to extract the information in the same call. This way you can have a more modular approach to the data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c4bc3abaf0fea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define separate function descriptions for each property.\n",
    "title_extraction_function_description = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"extract_title\",\n",
    "        \"description\": \"Extract the title from markdown text.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"title\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The title of the research article.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"title\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "authors_extraction_function_description = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"extract_authors\",\n",
    "        \"description\": \"Extract the list of authors from markdown text.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"authors\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                    \"description\": \"A list of authors who contributed to the article.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"authors\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "abstract_extraction_function_description = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"extract_abstract\",\n",
    "        \"description\": \"Extract the abstract from markdown text.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"abstract\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A brief summary of the article's content and findings.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"abstract\"],\n",
    "        },\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1635be75-f0df-4e0f-807e-7e50c40b85b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_completion_multiple_tool_calls(message: str):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": message}],\n",
    "        tools=[\n",
    "            title_extraction_function_description,\n",
    "            authors_extraction_function_description,\n",
    "            abstract_extraction_function_description,\n",
    "        ],\n",
    "        tool_choice=\"auto\",\n",
    "    )\n",
    "\n",
    "    # Combine the outputs from each function call.\n",
    "    extracted_data = {}\n",
    "    tool_calls = response.choices[0].message.tool_calls\n",
    "    for tool_call in tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        # Parse the JSON string of arguments.\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        if function_name == \"extract_title\":\n",
    "            extracted_data[\"title\"] = arguments[\"title\"]\n",
    "        elif function_name == \"extract_authors\":\n",
    "            extracted_data[\"authors\"] = arguments[\"authors\"]\n",
    "        elif function_name == \"extract_abstract\":\n",
    "            extracted_data[\"abstract\"] = arguments[\"abstract\"]\n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fedf482ed40fb5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example prompt that provides markdown text.\n",
    "prompt = f\"\"\"\n",
    "You are a document processing assistant. I have extracted the following markdown text from a PDF.\n",
    "Please use the given tools to extract the following details:\n",
    "- Title\n",
    "- Authors\n",
    "- Abstract\n",
    "\n",
    "Markdown text:\n",
    "{markdown_text}\n",
    "\"\"\"\n",
    "\n",
    "extracted_data = generate_completion_multiple_tool_calls(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd60bb02aa0ca9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a5e052-a9fd-4e70-bbc2-0acb0d09d6b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Cost\n",
    "\n",
    "Let's compute the cost of the completion. We'll use the following pricing: https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d43065fa170603",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_chatgpt_4o_cost(completion, verbose: bool = False) -> float:\n",
    "    input_tokens = completion.usage.prompt_tokens\n",
    "    output_tokens = completion.usage.completion_tokens\n",
    "\n",
    "    cost_per_1M_input_tokens = 0.15\n",
    "    cost_per_1M_output_tokens = 0.60\n",
    "\n",
    "    total_cost = (input_tokens / 1e6) * cost_per_1M_input_tokens\n",
    "    total_cost += (output_tokens / 1e6) * cost_per_1M_output_tokens\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Total input tokens: {input_tokens}\")\n",
    "        print(f\"Total output tokens: {output_tokens}\")\n",
    "        print(f\"Total tokens: {input_tokens+output_tokens}\")\n",
    "        print(f\"Estimated cost: ${total_cost:.4f}\")\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b280d6-cae1-41c2-85e1-65ce975c611f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_chatgpt_4o_cost(completion, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a8553f3ef6b06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "As you can see the major part of the cost is the input tokens. Here we're passing the whole document to the llm which make up for more than 90% of the cost. In the next part we'll see how to reduce the cost by passing only the relevant information to the llm (RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f8a80e-28aa-4b91-adbc-5a0a46cf40b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Structured output help the LLM to produce better and more interpretable results. On the chart below you'll find the relative performances in terms of reliability of the output matching the expected json format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2232c3abb0b6a8d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "![output_reliability](images/output_reliability.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd043297d27a185f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f5bac63d162b4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Exercise 1: Update the different methods to also extract the DOI\n",
    "\n",
    "Guideline:\n",
    "* Ask for the DOI id with the link.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c19a0ef3e18889",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Exercise 2: Extract the Bibliography.\n",
    "\n",
    "Guideline:\n",
    "* First define a json schema that will guide the data extraction.\n",
    "* Then define a tool call that will extract the data from one cited paper.\n",
    "* Finally call the tool multiple time and aggregate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8260d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
