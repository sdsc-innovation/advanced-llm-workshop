{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Textual RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![RAG Image](../data/rag.png)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is an AI technique that combines information retrieval with text generation. Instead of relying solely on a pre-trained language model’s internal knowledge, RAG dynamically retrieves relevant documents from an external knowledge base before generating a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![Why RAG Image](../data/why_rag.png)\n",
    "\n",
    "1. **Improved Accuracy:** RAG enhances the factual correctness of generated responses by retrieving up-to-date and domain-specific information, reducing the likelihood of hallucinations (fabricated information).\n",
    "\n",
    "2. **Better Generalization:** Since RAG dynamically retrieves relevant documents, it performs well across various domains without requiring extensive fine-tuning, making it more adaptable to new topics.\n",
    "\n",
    "3. **Reduced Model Size Requirements:** Instead of embedding all knowledge within a large model, RAG leverages external databases, allowing for smaller, more efficient models while maintaining high-quality responses.\n",
    "\n",
    "4. **Enhanced Explainability:** By referencing retrieved documents, RAG provides verifiable sources for its answers, making it more transparent and easier to trust compared to purely generative models.\n",
    "\n",
    "5. **And more...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In this exercise, you will learn how to implement a Retrieval-Augmented Generation (RAG) pipeline from scratch, without relying on tools like `langchain`. While `langchain` is a powerful framework that simplifies the development of RAG pipelines, it can sometimes lack flexibility for custom implementations, as it abstracts many components.\n",
    "\n",
    "The different components of the pipeline are:  \n",
    "\n",
    "- **Text extraction from PDFs** – Extract raw text from PDF files to make the content processable.  \n",
    "- **Text chunking** – Break the extracted text into smaller, meaningful segments to improve retrieval efficiency.  \n",
    "- **Embedding of the chunks** – Convert text chunks into numerical representations (embeddings) using a pre-trained model.  \n",
    "- **Storage of the embeddings in a vector store** – Save the embeddings in a specialized database (vector store) to enable fast similarity searches.  \n",
    "- **Relevant chunks retrieval** – Query the vector store to find the most relevant text chunks based on user input.  \n",
    "- **Setting and prompting of the LLM for a RAG** – Structure prompts and configure the language model to integrate retrieved information into its responses.  \n",
    "- **Additional tools for improved retrieval** – Use techniques like query expansion to reformulate user queries for better recall and reciprocal rank fusion to combine results from multiple retrieval methods.  \n",
    "- **Final RAG pipeline implementation** – Integrate all components into a complete system that retrieves relevant information and generates enhanced responses using the language model.  \n",
    "\n",
    "**Note:** To complete this exercise, you need an OpenAI API key, the PDF files, and the necessary libraries installed (see `requirements.txt`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "import chromadb\n",
    "\n",
    "from src.data_classes import Chunk\n",
    "from src.data_processing import SimpleChunker, PDFExtractorAPI\n",
    "from src.embedding import (\n",
    "    OpenAITextEmbeddings,\n",
    "    compute_openai_large_embedding_cost,\n",
    ")\n",
    "from src.vectorstore import (\n",
    "    ChromaDBVectorStore,\n",
    "    VectorStoreRetriever,\n",
    ")\n",
    "from src.llm import OpenAILLM\n",
    "from src.rag import Generator, DefaultRAG, query_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"../data\"\n",
    "\n",
    "pdf_files = [\n",
    "    \"Explainable_machine_learning_prediction_of_edema_a.pdf\",\n",
    "    \"Modeling tumor size dynamics based on real‐world electronic health records.pdf\",\n",
    "]\n",
    "example_pdf_file = \"Explainable_machine_learning_prediction_of_edema_a.pdf\"\n",
    "example_pdf_path = os.path.join(data_folder, example_pdf_file)\n",
    "\n",
    "vector_store_collection = \"text_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Example\n",
    "\n",
    "The example uses only `Explainable_machine_learning_prediction_of_edema_a.pdf`. Please, have a quick look at it before starting the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_question = \"According to SHAP analysis, which factors were the most influential in predicting higher-grade edema (Grade 2+)?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## LLM  \n",
    "\n",
    "The LLM is the core of the RAG system, responsible for generating responses based on the retrieved information. There are many options available on-premise or online, each with different performance, speed, specialized knowledge and cost trade-offs. In this case, we use `gpt-4o-mini`.  \n",
    "\n",
    "This LLM expects input in the form of a list of messages, where each message includes the content and the role of the speaker (e.g., system, user, assistant).  \n",
    "\n",
    "Here is how they are defined here:\n",
    "\n",
    "```python\n",
    "class Roles(str, Enum):\n",
    "    SYSTEM = \"system\"\n",
    "    USER = \"user\"\n",
    "    ASSISTANT = \"assistant\"\n",
    "    TOOL = \"tool\"\n",
    "\n",
    "class LLMMessage(BaseModel):\n",
    "    content: Optional[str] = None\n",
    "    role: Optional[Roles] = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAILLM(temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, price = llm.generate([{\"role\": \"user\", \"content\": test_question}], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## PDF Text Extraction  \n",
    "\n",
    "The first step in the pipeline is to extract text from the document.  \n",
    "\n",
    "In this exercise, we use the `MinerU` library, which under the hood uses among others `doclayout_yolo` for segmentation. Note that this model is not commercially permissive.\n",
    "\n",
    "The choice of extraction tool should be carefully considered. Depending on the document type and formatting, different methods may be required to preserve text integrity and leverage structural elements such as headings, tables, or metadata for better processing (`pdfplumber` (better for tables), `Tesseract OCR` (for scanned PDFs), ect.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_extractor = PDFExtractorAPI()\n",
    "_, text, _ = data_extractor.extract_text_and_images(example_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Text Chunking  \n",
    "\n",
    "The second step is to split the extracted text into smaller chunks, which will later be embedded and retrieved efficiently.  \n",
    "\n",
    "In this exercise, we use a simple heuristic approach: the text is split iteratively—first by heading levels (`#`), then by line breaks (`\\n`), and finally by sentence (`.`). Splitting only occurs if the resulting chunk exceeds a predefined length. However, more advanced techniques exist, such as **semantic chunking** (which splits based on meaning rather than syntax) or **agentic chunking** (which dynamically adapts chunk sizes based on context).  \n",
    "\n",
    "Each chunk is enriched with metadata, including:  \n",
    "- **Source file** – The document from which the chunk originates.  \n",
    "- **Chunk counter** – The position of the chunk within the file.  \n",
    "- **Unique identifier (`chunk_id`)** – Ensures each chunk can be referenced independently.  \n",
    "\n",
    "Additional metadata could be included to enable more refined filtering and retrieval strategies.  \n",
    "\n",
    "Here, our chunks are defined as:\n",
    "```python\n",
    "class Chunk(BaseModel):\n",
    "    chunk_id: int\n",
    "    content: str\n",
    "    metadata: dict = Field(default_factory=dict)\n",
    "    score: Optional[float] = None\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_metadata = {\"source_text\": example_pdf_file}\n",
    "\n",
    "text_chunker = SimpleChunker(max_chunk_size=1000)\n",
    "\n",
    "chunks = text_chunker.chunk_text(text, file_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(chunks))\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Embedding Model  \n",
    "\n",
    "Once the text is split into chunks, each chunk is converted into a numerical representation (embedding) that captures its meaning.  \n",
    "\n",
    "Here, we use OpenAI’s `text-embedding-3-large`, but other options exist, each with different trade-offs in on-premise vs online, accuracy, speed, and cost. The choice of model depends on the specific needs of the retrieval task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = compute_openai_large_embedding_cost(chunks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = OpenAITextEmbeddings()\n",
    "embeddings = embedding_model.get_embedding([chunk.content for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(embeddings.shape)\n",
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Vector Store and Retriever  \n",
    "\n",
    "After embedding the chunks, they need to be stored for efficient retrieval. The choice of vector store depends on factors like accuracy, speed, and filtering options. In this exercise, we use `ChromaDB`.  \n",
    "\n",
    "The next step is retrieving the most relevant chunks based on a query. In this implementation, the retriever uses only embeddings (sparse search). However, in some cases, dense search methods like BM25 or hybrid approaches combining both sparse and dense search can be used for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store = ChromaDBVectorStore(vector_store_collection)\n",
    "vector_store.insert_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = VectorStoreRetriever(embedding_model, vector_store)\n",
    "results = retriever.retrieve(test_question, 5)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Generator  \n",
    "\n",
    "Once the LLM is set up, a specific prompt needs to be defined for the RAG system. This prompt must include the retrieved chunks as context. The prompt has to be adapted to each specific project.\n",
    "\n",
    "In addition to the basic prompt, we incorporate **prompt engineering** by asking the LLM to justify its answer. The model is also instructed to indicate which chunks were most relevant in forming its response, improving **interpretability**, and to provide the answer in **JSON format** for easier data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_system_prompt = \"\"\"You are a helpful assistant, and your task is to answer questions using relevant documents. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\"\"\"\n",
    "print(default_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_rag_template = \"\"\"\n",
    "Here are the relevant DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Here is the USER QUESTION:\n",
    "{query}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Please think step-by-step and generate your output in json:\n",
    "\"\"\"\n",
    "print(default_rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = Generator(\n",
    "    llm, system_prompt=default_system_prompt, rag_template=default_rag_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, cost = generator.generate(\n",
    "    history=[],\n",
    "    query=test_question,\n",
    "    chunks=[\n",
    "        results[0][0][\"chunk\"],\n",
    "        Chunk(chunk_id=1, content=\"DATE: 1999.12.02\", metadata={}),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## RAG Tools  \n",
    "\n",
    "There are several methods to improve the efficiency of a RAG pipeline, such as query contextualization, query reformulation, re-ranking, query expansion, etc.\n",
    "\n",
    "In this notebook, we implement **query expansion** to enhance retrieval and apply **reciprocal rank fusion** to optimize the ranking of chunks when multiple queries are involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_expansion_system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a focused assistant designed to generate multiple, relevant search queries based solely on a single input query. Your task is to produce a list of these queries in English, without adding any further explanations or information.\",\n",
    "}\n",
    "\n",
    "query_expansion_template_query = \"\"\"\n",
    "        Generate multiple search queries related to: {query}, and translate them in english if they are not already in english. Only output {expansion_number} queries in english.\n",
    "        OUTPUT ({expansion_number} queries):\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, cost = query_expansion(\n",
    "    test_question,\n",
    "    llm,\n",
    "    query_expansion_system_message,\n",
    "    template_query_expansion=query_expansion_template_query,\n",
    "    expansion_number=5,\n",
    ")\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## RAG  \n",
    "\n",
    "Finally, the RAG pipeline is defined by integrating all the previously discussed components into a unified process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag = DefaultRAG(\n",
    "    llm=llm,\n",
    "    text_embedding_model=embedding_model,\n",
    "    text_vector_store=vector_store,\n",
    "    generator=generator,\n",
    "    query_expansion_system_message=query_expansion_system_message,\n",
    "    query_expansion_template_query=query_expansion_template_query,\n",
    "    params={\"top_k\": 5, \"number_query_expansion\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, sources, cost = rag.execute(test_question, {}, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The documents retrieved by the retriever:\n",
    "print(len(sources))\n",
    "print(sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Exercises\n",
    "\n",
    "The different blocks are redefined below, and a new pipeline is created that uses both PDFs.\n",
    "\n",
    "1. Quickly go through the code and the notebook above to ensure you understand how each block works.\n",
    "2. Answer the following questions related to `Explainable_machine_learning_prediction_of_edema_a.pdf` and analyze the answers:\n",
    "   1. \"What was identified as the most important predictor for edema occurrence?\"\n",
    "   2. \"Which machine learning algorithm performed best for predicting edema, and what was its F1 score?\"\n",
    "   3. \"How did cumulative tepotinib dose impact edema predictions, and what insights did SHAP provide about this relationship?\"\n",
    "   4. Propose your own question.\n",
    "3. Review the `Modeling tumor size dynamics based on real‐world electronic health records.pdf` and come up with a question. Ask it and analyze the answer, confirm that the retriever uses relevant chunks from this source.\n",
    "4. Discuss how the pipeline could be improved to achieve better answers. If time permits, implement those changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_extractor = PDFExtractorAPI()\n",
    "text_chunker = SimpleChunker(max_chunk_size=1000)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(data_folder, pdf_file)\n",
    "    _, text, _ = data_extractor.extract_text_and_images(pdf_path)\n",
    "    chunks_curr = text_chunker.chunk_text(text, {\"source_text\": pdf_file})\n",
    "    chunks.extend(chunks_curr)\n",
    "    print(len(chunks))\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = compute_openai_large_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = OpenAITextEmbeddings()\n",
    "embeddings = embedding_model.get_embedding([chunk.content for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset previous\n",
    "client = chromadb.Client()\n",
    "client.delete_collection(vector_store_collection)\n",
    "\n",
    "# Create new one\n",
    "vector_store = ChromaDBVectorStore(vector_store_collection)\n",
    "vector_store.insert_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = OpenAILLM(temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant, and your task is to answer questions using relevant documents. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\"\"\"\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_template = \"\"\"\n",
    "Here are the relevant DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Here is the USER QUESTION:\n",
    "{query}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Please think step-by-step and generate your output in json:\n",
    "\"\"\"\n",
    "print(rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_expansion_system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a focused assistant designed to generate multiple, relevant search queries based solely on a single input query. Your task is to produce a list of these queries in English, without adding any further explanations or information.\",\n",
    "}\n",
    "\n",
    "query_expansion_template_query = \"\"\"\n",
    "        Generate multiple search queries related to: {query}, and translate them in english if they are not already in english. Only output {expansion_number} queries in english.\n",
    "        OUTPUT ({expansion_number} queries):\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = Generator(llm, system_prompt=system_prompt, rag_template=rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag = DefaultRAG(\n",
    "    llm=llm,\n",
    "    text_embedding_model=embedding_model,\n",
    "    text_vector_store=vector_store,\n",
    "    generator=generator,\n",
    "    query_expansion_system_message=query_expansion_system_message,\n",
    "    query_expansion_template_query=query_expansion_template_query,\n",
    "    params={\"top_k\": 1, \"number_query_expansion\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, sources, cost = rag.execute(\n",
    "    \"Here goes my amazing question!\",\n",
    "    {},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The documents retrieved by the retriever:\n",
    "print(len(sources))\n",
    "print(sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-llm-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
