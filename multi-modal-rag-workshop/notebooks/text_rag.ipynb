{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Textual RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![RAG Image](../data/rag.png)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is an AI technique that combines information retrieval with text generation. Instead of relying solely on a pre-trained language model’s internal knowledge, RAG dynamically retrieves relevant documents from an external knowledge base before generating a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![Why RAG Image](../data/why_rag.png)\n",
    "\n",
    "1. **Improved Accuracy:** RAG enhances the factual correctness of generated responses by retrieving up-to-date and domain-specific information, reducing the likelihood of hallucinations (fabricated information).\n",
    "\n",
    "2. **Better Generalization:** Since RAG dynamically retrieves relevant documents, it performs well across various domains without requiring extensive fine-tuning, making it more adaptable to new topics.\n",
    "\n",
    "3. **Reduced Model Size Requirements:** Instead of embedding all knowledge within a large model, RAG leverages external databases, allowing for smaller, more efficient models while maintaining high-quality responses.\n",
    "\n",
    "4. **Enhanced Explainability:** By referencing retrieved documents, RAG provides verifiable sources for its answers, making it more transparent and easier to trust compared to purely generative models.\n",
    "\n",
    "5. **And more...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In this exercise, you will learn how to implement a Retrieval-Augmented Generation (RAG) pipeline from scratch, without relying on tools like `langchain`. While `langchain` is a powerful framework that simplifies the development of RAG pipelines, it can sometimes lack flexibility for custom implementations, as it abstracts many components.\n",
    "\n",
    "The different components of the pipeline are:  \n",
    "\n",
    "- **Text extraction from PDFs** – Extract raw text from PDF files to make the content processable.  \n",
    "- **Text chunking** – Break the extracted text into smaller, meaningful segments to improve retrieval efficiency.  \n",
    "- **Embedding of the chunks** – Convert text chunks into numerical representations (embeddings) using a pre-trained model.  \n",
    "- **Storage of the embeddings in a vector store** – Save the embeddings in a specialized database (vector store) to enable fast similarity searches.  \n",
    "- **Relevant chunks retrieval** – Query the vector store to find the most relevant text chunks based on user input.  \n",
    "- **Setting and prompting of the LLM for a RAG** – Structure prompts and configure the language model to integrate retrieved information into its responses.  \n",
    "- **Additional tools for improved retrieval** – Use techniques like query expansion to reformulate user queries for better recall and reciprocal rank fusion to combine results from multiple retrieval methods.  \n",
    "- **Final RAG pipeline implementation** – Integrate all components into a complete system that retrieves relevant information and generates enhanced responses using the language model.  \n",
    "\n",
    "**Note:** To complete this exercise, you need an OpenAI API key, the PDF files, and the necessary libraries installed (see `requirements.txt`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from -r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from -r requirements.txt (line 3)) (2.10.6)\n",
      "Requirement already satisfied: openai in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from -r requirements.txt (line 4)) (1.65.5)\n",
      "Requirement already satisfied: chromadb in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from -r requirements.txt (line 5)) (0.6.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from -r requirements.txt (line 6)) (3.10.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from -r requirements.txt (line 7)) (0.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from pydantic->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from pydantic->-r requirements.txt (line 3)) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from pydantic->-r requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from openai->-r requirements.txt (line 4)) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from openai->-r requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from openai->-r requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from openai->-r requirements.txt (line 4)) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from openai->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from openai->-r requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (0.115.11)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 5)) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (3.19.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (1.30.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (1.70.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (0.15.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (3.10.15)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from chromadb->-r requirements.txt (line 5)) (13.9.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (3.2.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from tiktoken->-r requirements.txt (line 7)) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from tiktoken->-r requirements.txt (line 7)) (2.32.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\sieverin\\appdata\\roaming\\python\\python310\\site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 5)) (7.0.1)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from fastapi>=0.95.2->chromadb->-r requirements.txt (line 5)) (0.46.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 5)) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 5)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 5)) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 5)) (0.9)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 5)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 5)) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 5)) (5.29.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 5)) (1.13.3)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 5)) (1.2.18)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 5)) (1.69.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 5)) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.30.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 5)) (1.30.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.51b0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 5)) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.51b0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 5)) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 5)) (0.51b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.51b0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 5)) (0.51b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 5)) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 5)) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 5)) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 7)) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 5)) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from tokenizers>=0.13.2->chromadb->-r requirements.txt (line 5)) (0.29.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 5)) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 5)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 5)) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 5)) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 5)) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 5)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 5)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 5)) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirements.txt (line 5)) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirements.txt (line 5)) (2025.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from importlib-metadata>=4.6->build>=1.0.3->chromadb->-r requirements.txt (line 5)) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 5)) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 5)) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\sieverin\\anaconda3\\envs\\advanced-llm-workshop\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 5)) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "import chromadb\n",
    "\n",
    "from src.data_classes import Chunk\n",
    "from src.data_processing import SimpleChunker, PDFExtractorAPI\n",
    "from src.embedding import (\n",
    "    OpenAITextEmbeddings,\n",
    "    compute_openai_large_embedding_cost,\n",
    ")\n",
    "from src.vectorstore import (\n",
    "    ChromaDBVectorStore,\n",
    "    VectorStoreRetriever,\n",
    ")\n",
    "from src.llm import OpenAILLM\n",
    "from src.rag import Generator, DefaultRAG, query_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"../data\"\n",
    "\n",
    "pdf_files = [\n",
    "    \"Explainable_machine_learning_prediction_of_edema_a.pdf\",\n",
    "    \"Modeling tumor size dynamics based on real‐world electronic health records.pdf\",\n",
    "]\n",
    "example_pdf_file = \"Explainable_machine_learning_prediction_of_edema_a.pdf\"\n",
    "example_pdf_path = os.path.join(data_folder, example_pdf_file)\n",
    "\n",
    "vector_store_collection = \"text_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Example\n",
    "\n",
    "The example uses only `Explainable_machine_learning_prediction_of_edema_a.pdf`. Please, have a quick look at it before starting the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_question = \"According to SHAP analysis, which factors were the most influential in predicting higher-grade edema (Grade 2+)?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## LLM  \n",
    "\n",
    "The LLM is the core of the RAG system, responsible for generating responses based on the retrieved information. There are many options available on-premise or online, each with different performance, speed, specialized knowledge and cost trade-offs. In this case, we use `gpt-4o-mini`.  \n",
    "\n",
    "This LLM expects input in the form of a list of messages, where each message includes the content and the role of the speaker (e.g., system, user, assistant).  \n",
    "\n",
    "Here is how they are defined here:\n",
    "\n",
    "```python\n",
    "class Roles(str, Enum):\n",
    "    SYSTEM = \"system\"\n",
    "    USER = \"user\"\n",
    "    ASSISTANT = \"assistant\"\n",
    "    TOOL = \"tool\"\n",
    "\n",
    "class LLMMessage(BaseModel):\n",
    "    content: Optional[str] = None\n",
    "    role: Optional[Roles] = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI LLM loaded: gpt-4o-mini; temperature: 0.5; seed: 42\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAILLM(temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to SHAP analysis, which factors were the most influential in predicting higher-grade edema (Grade 2+)?\n"
     ]
    }
   ],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input tokens: 30\n",
      "Total output tokens: 362\n",
      "Total tokens: 392\n",
      "Estimated cost: $0.0002\n"
     ]
    }
   ],
   "source": [
    "answer, price = llm.generate([{\"role\": \"user\", \"content\": test_question}], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP (SHapley Additive exPlanations) analysis is a method used to interpret the output of machine learning models by assigning each feature an importance value for a particular prediction. While I don't have access to specific datasets or studies conducted after October 2023, I can provide general guidance on how to interpret SHAP analysis results for predicting higher-grade edema (Grade 2+).\n",
      "\n",
      "In a typical SHAP analysis for predicting medical conditions like edema, the most influential factors may include:\n",
      "\n",
      "1. **Demographic Factors**: Age, sex, and ethnicity could play a significant role in the risk of developing higher-grade edema.\n",
      "\n",
      "2. **Clinical History**: Previous medical history, including conditions like heart failure, kidney disease, or liver dysfunction, may be critical.\n",
      "\n",
      "3. **Medication Use**: Certain medications, such as those that affect fluid retention (e.g., corticosteroids, NSAIDs), might be influential.\n",
      "\n",
      "4. **Biomarkers**: Laboratory results, such as levels of electrolytes, kidney function tests (e.g., creatinine), and inflammatory markers, can be significant predictors.\n",
      "\n",
      "5. **Imaging Findings**: Results from imaging studies, such as MRI or ultrasound, may provide insights into the severity of edema.\n",
      "\n",
      "6. **Lifestyle Factors**: Factors like obesity, physical activity level, and dietary habits could also influence edema severity.\n",
      "\n",
      "7. **Comorbidities**: The presence of other health conditions, such as diabetes or hypertension, might be associated with higher-grade edema.\n",
      "\n",
      "To identify the specific factors that were most influential in a particular SHAP analysis for predicting higher-grade edema, you would need to refer to the results of that specific analysis or study. The SHAP values would provide a ranked list of features based on their contribution to the model's predictions.\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## PDF Text Extraction  \n",
    "\n",
    "The first step in the pipeline is to extract text from the document.  \n",
    "\n",
    "In this exercise, we use the `MinerU` library, which under the hood uses among others `doclayout_yolo` for segmentation. Note that this model is not commercially permissive.\n",
    "\n",
    "The choice of extraction tool should be carefully considered. Depending on the document type and formatting, different methods may be required to preserve text integrity and leverage structural elements such as headings, tables, or metadata for better processing (`pdfplumber` (better for tables), `Tesseract OCR` (for scanned PDFs), ect.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_extractor = PDFExtractorAPI()\n",
    "_, text, _ = data_extractor.extract_text_and_images(example_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOI: [10.1111/cts.70010](https://doi.org/10.1111/cts.70010)\n",
      "\n",
      "### **ARTICLE**\n",
      "\n",
      "![](_page_0_Picture_4.jpeg)\n",
      "\n",
      "# **Explainable machine learning prediction of edema adverse events in patients treated with tepotinib**\n",
      "\n",
      "**Federico Amato[1](#page-0-0)** | **Rainer Strotmann[2](#page-0-1)** | **Roberto Castell[o1](#page-0-0)** | **Rolf Bruns[2](#page-0-1)** | **Vishal Ghori[3](#page-0-2)** | **Andreas John[e2](#page-0-1)** | **Karin Berghoff[2](#page-0-1)** | **Karthik Venkatakrishna[n4](#page-0-3)** | **Nadia Terranova[5](#page-0-4)**\n",
      "\n",
      "<span id=\"page-0-0\"></span>1 Swiss Data Science Center (EPFL and ETH Zurich), Lausanne, Switzerland\n",
      "\n",
      "<span id=\"page-0-1\"></span>2 The healthcare business of Merck KGaA, Darmstadt, Germany\n",
      "\n",
      "<span id=\"page-0-2\"></span>3 Ares Trading S.A., Eysins, Switzerland, an affiliate of Merck KGaA, Darmstadt, Germany\n",
      "\n",
      "<span id=\"page-0-3\"></span>4 EMD Serono, Billerica, Massachusetts, USA\n",
      "\n",
      "<span id=\"page-0-4\"></span>5 Quantitative Pharmacology, Ares Trading S.A., Lausanne, Swi\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Text Chunking  \n",
    "\n",
    "The second step is to split the extracted text into smaller chunks, which will later be embedded and retrieved efficiently.  \n",
    "\n",
    "In this exercise, we use a simple heuristic approach: the text is split iteratively—first by heading levels (`#`), then by line breaks (`\\n`), and finally by sentence (`.`). Splitting only occurs if the resulting chunk exceeds a predefined length. However, more advanced techniques exist, such as **semantic chunking** (which splits based on meaning rather than syntax) or **agentic chunking** (which dynamically adapts chunk sizes based on context).  \n",
    "\n",
    "Each chunk is enriched with metadata, including:  \n",
    "- **Source file** – The document from which the chunk originates.  \n",
    "- **Chunk counter** – The position of the chunk within the file.  \n",
    "- **Unique identifier (`chunk_id`)** – Ensures each chunk can be referenced independently.  \n",
    "\n",
    "Additional metadata could be included to enable more refined filtering and retrieval strategies.  \n",
    "\n",
    "Here, our chunks are defined as:\n",
    "```python\n",
    "class Chunk(BaseModel):\n",
    "    chunk_id: int\n",
    "    content: str\n",
    "    metadata: dict = Field(default_factory=dict)\n",
    "    score: Optional[float] = None\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_metadata = {\"source_text\": example_pdf_file}\n",
    "\n",
    "text_chunker = SimpleChunker(max_chunk_size=1000)\n",
    "\n",
    "chunks = text_chunker.chunk_text(text, file_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chunk(chunk_id=0, content='DOI: [10.1111/cts.70010](https://doi.org/10.1111/cts.70010) ### **ARTICLE** ![](_page_0_Picture_4.jpeg) # **Explainable machine learning prediction of edema adverse events in patients treated with tepotinib** **Federico Amato[1](#page-0-0)** | **Rainer Strotmann[2](#page-0-1)** | **Roberto Castell[o1](#page-0-0)** | **Rolf Bruns[2](#page-0-1)** | **Vishal Ghori[3](#page-0-2)** | **Andreas John[e2](#page-0-1)** | **Karin Berghoff[2](#page-0-1)** | **Karthik Venkatakrishna[n4](#page-0-3)** | **Nadia Terranova[5](#page-0-4)** <span id=\"page-0-0\"></span>1 Swiss Data Science Center (EPFL and ETH Zurich), Lausanne, Switzerland <span id=\"page-0-1\"></span>2 The healthcare business of Merck KGaA, Darmstadt, Germany <span id=\"page-0-2\"></span>3 Ares Trading S.A., Eysins, Switzerland, an affiliate of Merck KGaA, Darmstadt, Germany <span id=\"page-0-3\"></span>4 EMD Serono, Billerica, Massachusetts, USA', metadata={'source_text': 'Explainable_machine_learning_prediction_of_edema_a.pdf', 'document_chunk_id': 0}, data_type=<DataType.TEXT: 'text'>, score=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(chunks))\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Embedding Model  \n",
    "\n",
    "Once the text is split into chunks, each chunk is converted into a numerical representation (embedding) that captures its meaning.  \n",
    "\n",
    "Here, we use OpenAI’s `text-embedding-3-large`, but other options exist, each with different trade-offs in on-premise vs online, accuracy, speed, and cost. The choice of model depends on the specific needs of the retrieval task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 13665\n",
      "Estimated cost: $0.0018\n"
     ]
    }
   ],
   "source": [
    "_ = compute_openai_large_embedding_cost(chunks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = OpenAITextEmbeddings()\n",
    "embeddings = embedding_model.get_embedding([chunk.content for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 3072)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.03154377,  0.00651717, -0.01348095, ...,  0.00990808,\n",
       "        0.00328753, -0.00139772], shape=(3072,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embeddings.shape)\n",
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Vector Store and Retriever  \n",
    "\n",
    "After embedding the chunks, they need to be stored for efficient retrieval. The choice of vector store depends on factors like accuracy, speed, and filtering options. In this exercise, we use `ChromaDB`.  \n",
    "\n",
    "The next step is retrieving the most relevant chunks based on a query. In this implementation, the retriever uses only embeddings (sparse search). However, in some cases, dense search methods like BM25 or hybrid approaches combining both sparse and dense search can be used for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store = ChromaDBVectorStore(vector_store_collection)\n",
    "vector_store.insert_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to SHAP analysis, which factors were the most influential in predicting higher-grade edema (Grade 2+)?\n"
     ]
    }
   ],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'chunk_id': '39',\n",
       "   'score': 0.5416353940963745,\n",
       "   'chunk': Chunk(chunk_id=39, content='Points are colored based on the edema grade at the following safety visit. SHAP, Shapley Additive exPlanations. with higher grades of edema, particularly grade 2+. On the other hand, for higher albumin levels the corresponding SHAP values are mostly negative and ranging from 0 to −0.5, suggesting a reduced risk of edema of grade 2+. The association between age greater than 70years and an increased likelihood of edemas of grades 2+ was also confirmed. Additionally, for all ages, higher SHAP values were assigned to patients who experienced edemas, particularly of grade 2+. Finally, within low ranges of cumulated dose in the interval [ *t* − 14 days, *t* ] normalized over 14days, higher SHAP values were assigned to samples corresponding to edemas of grades 2+. This could reflect the tendency to adjust administered doses in those cases where the risk of edema was identified. # **DISCUSSION**', metadata={'source_text': 'Explainable_machine_learning_prediction_of_edema_a.pdf', 'document_chunk_id': 39}, data_type=<DataType.TEXT: 'text'>, score=None)},\n",
       "  {'chunk_id': '32',\n",
       "   'score': 0.6098582744598389,\n",
       "   'chunk': Chunk(chunk_id=32, content='Consistently with the above sensitivity analysis, past current edema grade was found to be the most influential input, particularly if a same grade persisted to the following safety visit. The exposure-derived features were also informative for the model probability predictions. Albumin was found as the most informative time-varying covariate, especially for predicting edemas of grades 2+. Figure [3](#page-7-1) illustrates the contribution of the input variables toward the predicted probability of edemas of grades 2+. The analysis reveals that the current edema grade is the most informative input, as patients with a history of edemas of grades 2+ are considered highly likely to experience the same grade in the future. Interestingly, albumin once again emerges as the most informative among the longitudinal covariates, with lower levels associated <span id=\"page-6-0\"></span>', metadata={'source_text': 'Explainable_machine_learning_prediction_of_edema_a.pdf', 'document_chunk_id': 32}, data_type=<DataType.TEXT: 'text'>, score=None)},\n",
       "  {'chunk_id': '42',\n",
       "   'score': 0.6162319183349609,\n",
       "   'chunk': Chunk(chunk_id=42, content='The presence of such input is ensuring high model performances when predicting persistence of a given edema grade. The sensitivity analysis to the inclusion of this input revealed a decline in the mean cross-validation F1 score of ~0.350 when it was removed as a candidate predictor. However, the use of Isotonic Regressions ensures the estimations of correctly calibrated probabilities. For instance, while low predicted probabilities, for example, for edemas of grade 2+, might lead to a classification error, they can still provide valuable details on existing risk of occurrence of an adverse event (Figure [S5\\\\)](#page-11-0). Moreover, sensitivity analysis together with the SHAP importance highlighted interesting patterns with respect to the exposure-related features. Referencing Figure [3](#page-7-1), lower values of the dose [ *t* − 14 days, *t* ] are associated with a decreased probability of edema of grades 2+ at subsequent visits.', metadata={'source_text': 'Explainable_machine_learning_prediction_of_edema_a.pdf', 'document_chunk_id': 42}, data_type=<DataType.TEXT: 'text'>, score=None)},\n",
       "  {'chunk_id': '37',\n",
       "   'score': 0.6610918045043945,\n",
       "   'chunk': Chunk(chunk_id=37, content='![](_page_7_Figure_0.jpeg) <span id=\"page-7-0\"></span>**FIGURE 2** Global input importance via mean SHAP values. Ranking of the model input for the most influential to the less influential for the model. The y-axis indicates the average change in the predicted probability of edema by grade, on average across the entire test set. SHAP, Shapley Additive exPlanations. <span id=\"page-7-1\"></span>**FIGURE 3** SHAP values – contribution of the inputs toward the predicted probabilities of edemas of grade 2+. List of the eight most influential inputs with respect to the predicted probabilities of edemas of grades 2+. Each point on the plot is a SHAP value for a covariate at a specific patient visit. The position on the y-axis indicates the covariate importance and on the x-axis the impact on the predicted probability. Color represents the value of the covariate. SHAP, Shapley Additive exPlanations. with an increase in the predicted probability of edemas of grades 2+.', metadata={'source_text': 'Explainable_machine_learning_prediction_of_edema_a.pdf', 'document_chunk_id': 37}, data_type=<DataType.TEXT: 'text'>, score=None)},\n",
       "  {'chunk_id': '45',\n",
       "   'score': 0.7389312386512756,\n",
       "   'chunk': Chunk(chunk_id=45, content=\"Advanced age was also found as predictive of edemas of grade 2+, in agreement with current knowledge of the investigated adverse event behavior, for which age is known to be a risk factor independently from drug exposure.[36,37](#page-11-11) Finally, the time until the next visit, used to inform the model about the forecasting horizon, was also informative. However, this input should be seen as a factor reflecting the deteriorating status of patients, as changes in medical condition could prompt clinicians to schedule short-term (re-)assessment visits. One of the primary challenges encountered in this study's classification setting was the unbalanced representation of different edema grades in the data. To overcome this issue, the model was set up to produce a probability for each edema grade of any new patient instance. When dealing with unbalanced data, such probabilities can be small, which is not a problem as long as they are accurate.\", metadata={'source_text': 'Explainable_machine_learning_prediction_of_edema_a.pdf', 'document_chunk_id': 45}, data_type=<DataType.TEXT: 'text'>, score=None)}]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = VectorStoreRetriever(embedding_model, vector_store)\n",
    "results = retriever.retrieve(test_question, 5)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Generator  \n",
    "\n",
    "Once the LLM is set up, a specific prompt needs to be defined for the RAG system. This prompt must include the retrieved chunks as context. The prompt has to be adapted to each specific project.\n",
    "\n",
    "In addition to the basic prompt, we incorporate **prompt engineering** by asking the LLM to justify its answer. The model is also instructed to indicate which chunks were most relevant in forming its response, improving **interpretability**, and to provide the answer in **JSON format** for easier data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant, and your task is to answer questions using relevant documents. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\n"
     ]
    }
   ],
   "source": [
    "default_system_prompt = \"\"\"You are a helpful assistant, and your task is to answer questions using relevant documents. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\"\"\"\n",
    "print(default_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are the relevant DOCUMENTS:\n",
      "{context}\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Here is the USER QUESTION:\n",
      "{query}\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Please think step-by-step and generate your output in json:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_rag_template = \"\"\"\n",
    "Here are the relevant DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Here is the USER QUESTION:\n",
    "{query}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Please think step-by-step and generate your output in json:\n",
    "\"\"\"\n",
    "print(default_rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to SHAP analysis, which factors were the most influential in predicting higher-grade edema (Grade 2+)?\n"
     ]
    }
   ],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = Generator(\n",
    "    llm, system_prompt=default_system_prompt, rag_template=default_rag_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant, and your task is to answer questions using relevant documents. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\n",
      "\n",
      "Here are the relevant DOCUMENTS:\n",
      "\n",
      "\n",
      "Document 1: \n",
      "Points are colored based on the edema grade at the following safety visit. SHAP, Shapley Additive exPlanations. with higher grades of edema, particularly grade 2+. On the other hand, for higher albumin levels the corresponding SHAP values are mostly negative and ranging from 0 to −0.5, suggesting a reduced risk of edema of grade 2+. The association between age greater than 70years and an increased likelihood of edemas of grades 2+ was also confirmed. Additionally, for all ages, higher SHAP values were assigned to patients who experienced edemas, particularly of grade 2+. Finally, within low ranges of cumulated dose in the interval [ *t* − 14 days, *t* ] normalized over 14days, higher SHAP values were assigned to samples corresponding to edemas of grades 2+. This could reflect the tendency to adjust administered doses in those cases where the risk of edema was identified. # **DISCUSSION**\n",
      "\n",
      "Document 2: \n",
      "DATE: 1999.12.02\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Here is the USER QUESTION:\n",
      "According to SHAP analysis, which factors were the most influential in predicting higher-grade edema (Grade 2+)?\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Please think step-by-step and generate your output in json:\n",
      "\n",
      "Total input tokens: 379\n",
      "Total output tokens: 167\n",
      "Total tokens: 546\n",
      "Estimated cost: $0.0002\n"
     ]
    }
   ],
   "source": [
    "answer, cost = generator.generate(\n",
    "    history=[],\n",
    "    query=test_question,\n",
    "    chunks=[\n",
    "        results[0][0][\"chunk\"],\n",
    "        Chunk(chunk_id=1, content=\"DATE: 1999.12.02\", metadata={}),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"step_by_step_thinking\": \"I reviewed Document 1 to identify the factors influencing higher-grade edema (Grade 2+). The document mentions that higher edema grades correlate with certain factors, particularly age greater than 70 years, higher SHAP values for patients with edema, and lower albumin levels. Additionally, it discusses the impact of administered doses normalized over 14 days, suggesting that adjustments in doses occur when the risk of edema is identified. Therefore, the most influential factors identified are age, albumin levels, and cumulated dose adjustments.\",\n",
      "  \"document_used\": [1],\n",
      "  \"answer\": \"The most influential factors in predicting higher-grade edema (Grade 2+) according to SHAP analysis are age greater than 70 years, lower albumin levels, and adjustments in cumulated doses.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## RAG Tools  \n",
    "\n",
    "There are several methods to improve the efficiency of a RAG pipeline, such as query contextualization, query reformulation, re-ranking, query expansion, etc.\n",
    "\n",
    "In this notebook, we implement **query expansion** to enhance retrieval and apply **reciprocal rank fusion** to optimize the ranking of chunks when multiple queries are involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_expansion_system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a focused assistant designed to generate multiple, relevant search queries based solely on a single input query. Your task is to produce a list of these queries in English, without adding any further explanations or information.\",\n",
    "}\n",
    "\n",
    "query_expansion_template_query = \"\"\"\n",
    "        Generate multiple search queries related to: {query}, and translate them in english if they are not already in english. Only output {expansion_number} queries in english.\n",
    "        OUTPUT ({expansion_number} queries):\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to SHAP analysis, which factors were the most influential in predicting higher-grade edema (Grade 2+)?\n"
     ]
    }
   ],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input tokens: 113\n",
      "Total output tokens: 89\n",
      "Total tokens: 202\n",
      "Estimated cost: $0.0001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1. What factors influence higher-grade edema according to SHAP analysis?  ',\n",
       " '2. How does SHAP analysis determine the predictors of Grade 2+ edema?  ',\n",
       " '3. Which variables are most significant in predicting higher-grade edema using SHAP?  ',\n",
       " '4. What insights does SHAP analysis provide on factors affecting Grade 2+ edema?  ',\n",
       " '5. Can SHAP analysis identify key predictors for severe edema (Grade 2+)?']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer, cost = query_expansion(\n",
    "    test_question,\n",
    "    llm,\n",
    "    query_expansion_system_message,\n",
    "    template_query_expansion=query_expansion_template_query,\n",
    "    expansion_number=5,\n",
    ")\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## RAG  \n",
    "\n",
    "Finally, the RAG pipeline is defined by integrating all the previously discussed components into a unified process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag = DefaultRAG(\n",
    "    llm=llm,\n",
    "    text_embedding_model=embedding_model,\n",
    "    text_vector_store=vector_store,\n",
    "    generator=generator,\n",
    "    query_expansion_system_message=query_expansion_system_message,\n",
    "    query_expansion_template_query=query_expansion_template_query,\n",
    "    params={\"top_k\": 5, \"number_query_expansion\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to SHAP analysis, which factors were the most influential in predicting higher-grade edema (Grade 2+)?\n"
     ]
    }
   ],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total input tokens: 113\n",
      "Total output tokens: 59\n",
      "Total tokens: 172\n",
      "Estimated cost: $0.0001\n",
      "Query expansion cost: 0.0001\n",
      "Expanded queries:\n",
      "1. What factors influence higher-grade edema (Grade 2+) according to SHAP analysis?  \n",
      "2. How does SHAP analysis identify key predictors for Grade 2+ edema?  \n",
      "3. Which variables are most significant in predicting Grade 2+ edema using SHAP analysis?\n",
      "\n",
      "You are a helpful assistant, and your task is to answer questions using relevant documents. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\n",
      "\n",
      "Here are the relevant DOCUMENTS:\n",
      "\n",
      "\n",
      "Document 1: \n",
      "Points are colored based on the edema grade at the following safety visit. SHAP, Shapley Additive exPlanations. with higher grades of edema, particularly grade 2+. On the other hand, for higher albumin levels the corresponding SHAP values are mostly negative and ranging from 0 to −0.5, suggesting a reduced risk of edema of grade 2+. The association between age greater than 70years and an increased likelihood of edemas of grades 2+ was also confirmed. Additionally, for all ages, higher SHAP values were assigned to patients who experienced edemas, particularly of grade 2+. Finally, within low ranges of cumulated dose in the interval [ *t* − 14 days, *t* ] normalized over 14days, higher SHAP values were assigned to samples corresponding to edemas of grades 2+. This could reflect the tendency to adjust administered doses in those cases where the risk of edema was identified. # **DISCUSSION**\n",
      "\n",
      "Document 2: \n",
      "![](_page_7_Figure_0.jpeg) <span id=\"page-7-0\"></span>**FIGURE 2** Global input importance via mean SHAP values. Ranking of the model input for the most influential to the less influential for the model. The y-axis indicates the average change in the predicted probability of edema by grade, on average across the entire test set. SHAP, Shapley Additive exPlanations. <span id=\"page-7-1\"></span>**FIGURE 3** SHAP values – contribution of the inputs toward the predicted probabilities of edemas of grade 2+. List of the eight most influential inputs with respect to the predicted probabilities of edemas of grades 2+. Each point on the plot is a SHAP value for a covariate at a specific patient visit. The position on the y-axis indicates the covariate importance and on the x-axis the impact on the predicted probability. Color represents the value of the covariate. SHAP, Shapley Additive exPlanations. with an increase in the predicted probability of edemas of grades 2+.\n",
      "\n",
      "Document 3: \n",
      "The presence of such input is ensuring high model performances when predicting persistence of a given edema grade. The sensitivity analysis to the inclusion of this input revealed a decline in the mean cross-validation F1 score of ~0.350 when it was removed as a candidate predictor. However, the use of Isotonic Regressions ensures the estimations of correctly calibrated probabilities. For instance, while low predicted probabilities, for example, for edemas of grade 2+, might lead to a classification error, they can still provide valuable details on existing risk of occurrence of an adverse event (Figure [S5\\)](#page-11-0). Moreover, sensitivity analysis together with the SHAP importance highlighted interesting patterns with respect to the exposure-related features. Referencing Figure [3](#page-7-1), lower values of the dose [ *t* − 14 days, *t* ] are associated with a decreased probability of edema of grades 2+ at subsequent visits.\n",
      "\n",
      "Document 4: \n",
      "The second objective of the study was the identification of the factors predicting edema occurrence and evolution over time. The Shapley Additive exPlanations (SHAP) method was used to investigate the role different factors have toward a specific estimation of edema occurrence obtained via the best predictive model, both at population and patient level. The use of this approach overcomes the lack of explainability of ML models, which approximate complex nonlinear functions from data in a not straightforwardly interpretable manner.[20,21](#page-10-11) # **METHODS** # **Clinical data** Data from 612 patients enrolled in five Phase I/II clinical studies with tepotinib were collected (NCT01014936, NCT01832506, NCT01988493, NCT02115373, VISION – NCT02864992).\n",
      "\n",
      "Document 5: \n",
      "Consistently with the above sensitivity analysis, past current edema grade was found to be the most influential input, particularly if a same grade persisted to the following safety visit. The exposure-derived features were also informative for the model probability predictions. Albumin was found as the most informative time-varying covariate, especially for predicting edemas of grades 2+. Figure [3](#page-7-1) illustrates the contribution of the input variables toward the predicted probability of edemas of grades 2+. The analysis reveals that the current edema grade is the most informative input, as patients with a history of edemas of grades 2+ are considered highly likely to experience the same grade in the future. Interestingly, albumin once again emerges as the most informative among the longitudinal covariates, with lower levels associated <span id=\"page-6-0\"></span>\n",
      "\n",
      "Document 6: \n",
      "Moreover, age impact on predictions appears to have an evident pattern, with older subjects associated with an increased probability of edemas of grade 2+. Figure [4](#page-8-0) illustrates the relationship between albumin, age, the [ *t* − 14 days, *t* ] cumulated dose normalized over 14days, and their corresponding SHAP values for predicting the likelihood of edemas of grades 2+. For lower albumin levels, positive SHAP contributions between 0 and 0.5 are consistently assigned, signifying an increased risk of developing edema of grade 2+. Notably, very low albumin values are predominantly associated ![](_page_8_Figure_1.jpeg) <span id=\"page-8-0\"></span>**FIGURE 4** Interactions between covariate values and corresponding SHAP values. Scatterplot of the covariate value against its corresponding SHAP value for albumin, age, and cumulated dose over 2weeks prior to the time at which prediction is performed. Each point corresponds to a specific patient visit.\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Here is the USER QUESTION:\n",
      "According to SHAP analysis, which factors were the most influential in predicting higher-grade edema (Grade 2+)?\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Please think step-by-step and generate your output in json:\n",
      "\n",
      "Total input tokens: 1395\n",
      "Total output tokens: 224\n",
      "Total tokens: 1619\n",
      "Estimated cost: $0.0003\n"
     ]
    }
   ],
   "source": [
    "answer, sources, cost = rag.execute(test_question, {}, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"step_by_step_thinking\": \"To determine the most influential factors in predicting higher-grade edema (Grade 2+), I reviewed the relevant documents. Document 5 indicates that the past current edema grade is the most influential input, especially if the same grade persists to the next safety visit. Additionally, Document 5 also highlights that albumin is the most informative time-varying covariate for predicting Grade 2+ edemas. Document 6 further supports this by showing that lower albumin levels are associated with an increased risk of developing Grade 2+ edema. Furthermore, Document 1 mentions that age greater than 70 years is associated with a higher likelihood of Grade 2+ edemas. Therefore, the key factors identified are: past current edema grade, albumin levels, and age over 70 years.\",\n",
      "   \"document_used\": [\n",
      "      1,\n",
      "      5,\n",
      "      6\n",
      "   ],\n",
      "   \"answer\": \"The most influential factors in predicting higher-grade edema (Grade 2+) are: past current edema grade, lower albumin levels, and age greater than 70 years.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "{'chunk_id': '39', 'chunk': Chunk(chunk_id=39, content='Points are colored based on the edema grade at the following safety visit. SHAP, Shapley Additive exPlanations. with higher grades of edema, particularly grade 2+. On the other hand, for higher albumin levels the corresponding SHAP values are mostly negative and ranging from 0 to −0.5, suggesting a reduced risk of edema of grade 2+. The association between age greater than 70years and an increased likelihood of edemas of grades 2+ was also confirmed. Additionally, for all ages, higher SHAP values were assigned to patients who experienced edemas, particularly of grade 2+. Finally, within low ranges of cumulated dose in the interval [ *t* − 14 days, *t* ] normalized over 14days, higher SHAP values were assigned to samples corresponding to edemas of grades 2+. This could reflect the tendency to adjust administered doses in those cases where the risk of edema was identified. # **DISCUSSION**', metadata={'source_text': 'Explainable_machine_learning_prediction_of_edema_a.pdf', 'document_chunk_id': 39}, data_type=<DataType.TEXT: 'text'>, score=None), 'fused_score': 0.04620592054390803, 'average_original_score': 0.5385304689407349}\n"
     ]
    }
   ],
   "source": [
    "# The documents retrieved by the retriever:\n",
    "print(len(sources))\n",
    "print(sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000396\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Exercises\n",
    "\n",
    "The different blocks are redefined below, and a new pipeline is created that uses both PDFs.\n",
    "\n",
    "1. Quickly go through the code and the notebook above to ensure you understand how each block works.\n",
    "2. Answer the following questions related to `Explainable_machine_learning_prediction_of_edema_a.pdf` and analyze the answers:\n",
    "   1. \"What was identified as the most important predictor for edema occurrence?\"\n",
    "   2. \"Which machine learning algorithm performed best for predicting edema, and what was its F1 score?\"\n",
    "   3. \"How did cumulative tepotinib dose impact edema predictions, and what insights did SHAP provide about this relationship?\"\n",
    "   4. Propose your own question.\n",
    "3. Review the `Modeling tumor size dynamics based on real‐world electronic health records.pdf` and come up with a question. Ask it and analyze the answer, confirm that the retriever uses relevant chunks from this source.\n",
    "4. Discuss how the pipeline could be improved to achieve better answers. If time permits, implement those changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_extractor = PDFExtractorAPI()\n",
    "text_chunker = SimpleChunker(max_chunk_size=1000)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(data_folder, pdf_file)\n",
    "    _, text, _ = data_extractor.extract_text_and_images(pdf_path)\n",
    "    chunks_curr = text_chunker.chunk_text(text, {\"source_text\": pdf_file})\n",
    "    chunks.extend(chunks_curr)\n",
    "    print(len(chunks))\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 27879\n",
      "Estimated cost: $0.0036\n"
     ]
    }
   ],
   "source": [
    "_ = compute_openai_large_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = OpenAITextEmbeddings()\n",
    "embeddings = embedding_model.get_embedding([chunk.content for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset previous\n",
    "client = chromadb.Client()\n",
    "client.delete_collection(vector_store_collection)\n",
    "\n",
    "# Create new one\n",
    "vector_store = ChromaDBVectorStore(vector_store_collection)\n",
    "vector_store.insert_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI LLM loaded: gpt-4o-mini; temperature: 1.0; seed: 42\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAILLM(temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant, and your task is to answer questions using relevant documents. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant, and your task is to answer questions using relevant documents. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\"\"\"\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are the relevant DOCUMENTS:\n",
      "{context}\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Here is the USER QUESTION:\n",
      "{query}\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Please think step-by-step and generate your output in json:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_template = \"\"\"\n",
    "Here are the relevant DOCUMENTS:\n",
    "{context}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Here is the USER QUESTION:\n",
    "{query}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Please think step-by-step and generate your output in json:\n",
    "\"\"\"\n",
    "print(rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_expansion_system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a focused assistant designed to generate multiple, relevant search queries based solely on a single input query. Your task is to produce a list of these queries in English, without adding any further explanations or information.\",\n",
    "}\n",
    "\n",
    "query_expansion_template_query = \"\"\"\n",
    "        Generate multiple search queries related to: {query}, and translate them in english if they are not already in english. Only output {expansion_number} queries in english.\n",
    "        OUTPUT ({expansion_number} queries):\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = Generator(llm, system_prompt=system_prompt, rag_template=rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag = DefaultRAG(\n",
    "    llm=llm,\n",
    "    text_embedding_model=embedding_model,\n",
    "    text_vector_store=vector_store,\n",
    "    generator=generator,\n",
    "    query_expansion_system_message=query_expansion_system_message,\n",
    "    query_expansion_template_query=query_expansion_template_query,\n",
    "    params={\"top_k\": 1, \"number_query_expansion\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant, and your task is to answer questions using relevant documents. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\n",
      "\n",
      "Here are the relevant DOCUMENTS:\n",
      "\n",
      "\n",
      "Document 1: \n",
      "<span id=\"page-11-1\"></span>Additional supporting information can be found online in the Supporting Information section at the end of this article. **How to cite this article:** Amato F, Strotmann R, Castello R, et al. Explainable machine learning prediction of edema adverse events in patients treated with tepotinib. *Clin Transl Sci*. 2024;17:e70010. doi[:10.1111/cts.70010](https://doi.org/10.1111/cts.70010)\n",
      "\n",
      "Document 2: \n",
      "DOI: [10.1111/cts.70010](https://doi.org/10.1111/cts.70010) ### **ARTICLE** ![](_page_0_Picture_4.jpeg) # **Explainable machine learning prediction of edema adverse events in patients treated with tepotinib** **Federico Amato[1](#page-0-0)** | **Rainer Strotmann[2](#page-0-1)** | **Roberto Castell[o1](#page-0-0)** | **Rolf Bruns[2](#page-0-1)** | **Vishal Ghori[3](#page-0-2)** | **Andreas John[e2](#page-0-1)** | **Karin Berghoff[2](#page-0-1)** | **Karthik Venkatakrishna[n4](#page-0-3)** | **Nadia Terranova[5](#page-0-4)** <span id=\"page-0-0\"></span>1 Swiss Data Science Center (EPFL and ETH Zurich), Lausanne, Switzerland <span id=\"page-0-1\"></span>2 The healthcare business of Merck KGaA, Darmstadt, Germany <span id=\"page-0-2\"></span>3 Ares Trading S.A., Eysins, Switzerland, an affiliate of Merck KGaA, Darmstadt, Germany <span id=\"page-0-3\"></span>4 EMD Serono, Billerica, Massachusetts, USA\n",
      "\n",
      "Document 3: \n",
      "*Clinical and Translational Science* published by Wiley Periodicals LLC on behalf of American Society for Clinical Pharmacology and Therapeutics. dose modifications in reducing this AE, exploring its relationship with potential prognostic factors. ### **WHAT QUESTION DID THIS STUDY ADDRESS?** Are there baseline and time-varying factors to support the identification of higher likelihood of edema occurrence in patients receiving tepotinib treatment? ### **WHAT DOES THIS STUDY ADD TO OUR KNOWLEDGE?** This study assesses 54 covariates as predictors of edema using ML. Explainability tools investigate the relationship between input covariates and predicted outcomes. The identified drivers align with the existing knowledge of the investigated AE behavior. ### **HOW MIGHT THIS CHANGE DRUG DISCOVERY, DEVELOPMENT, AND/OR THERAPEUTICS?**\n",
      "\n",
      "Document 4: \n",
      "Murphy KP. *Machine Learning – A Probabilistic Perspective*. The MIT Press; 2012. - 8. Sibieude E, Khandelwal A, Girard P, Hesthaven JS, Terranova N. Population pharmacokinetic model selection assisted by machine learning. *J Pharmacokinet Pharmacodyn*. 2022;49:257-270. - <span id=\"page-10-5\"></span>9. Baker RE, Peña JM, Jayamohan J, Jérusalem A. Mechanistic models versus machine learning, a fight worth fighting for the biological community? *Biol Lett*. 2018;14:20170660. - <span id=\"page-10-6\"></span>10. Xiong W, Hietala SF, Nyberg J, et al. Exposure-response analyses for the MET inhibitor tepotinib including patients in the pivotal VISION trial: support for dosage recommendations. *Cancer Chemother Pharmacol*. 2022;90:53-69. - <span id=\"page-10-7\"></span>11. Gao CF, Woude GF. Vande HGF/SF-met signaling in tumor progression. *Cell Res*. 2005;15:49-51. - 12. Birchmeier C, Birchmeier W, Gherardi E, Woude GF. Vande met, metastasis, motility and more. *Nat Rev Mol Cell Biol*.\n",
      "\n",
      "Document 5: \n",
      "![](_page_7_Figure_0.jpeg) <span id=\"page-7-0\"></span>**FIGURE 2** Global input importance via mean SHAP values. Ranking of the model input for the most influential to the less influential for the model. The y-axis indicates the average change in the predicted probability of edema by grade, on average across the entire test set. SHAP, Shapley Additive exPlanations. <span id=\"page-7-1\"></span>**FIGURE 3** SHAP values – contribution of the inputs toward the predicted probabilities of edemas of grade 2+. List of the eight most influential inputs with respect to the predicted probabilities of edemas of grades 2+. Each point on the plot is a SHAP value for a covariate at a specific patient visit. The position on the y-axis indicates the covariate importance and on the x-axis the impact on the predicted probability. Color represents the value of the covariate. SHAP, Shapley Additive exPlanations. with an increase in the predicted probability of edemas of grades 2+.\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Here is the USER QUESTION:\n",
      "Here goes my amazing question!\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Please think step-by-step and generate your output in json:\n",
      "\n",
      "Total input tokens: 1312\n",
      "Total output tokens: 193\n",
      "Total tokens: 1505\n",
      "Estimated cost: $0.0003\n"
     ]
    }
   ],
   "source": [
    "answer, sources, cost = rag.execute(\n",
    "    \"Here goes my amazing question!\",\n",
    "    {},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"step_by_step_thinking\": \"I analyzed the given documents to answer the question regarding the machine learning study on edema in patients treated with tepotinib. I focused particularly on Document 3, which discusses the specific study objectives, the factors evaluated (54 covariates), and the use of machine learning to assess predictors of edema. Additionally, Document 5 outlines the use of SHAP values to interpret the model's predictions, indicating how different inputs affect the prediction of edema severity. These details collectively illustrate how the study utilizes machine learning for assessing edemas in a clinical context.\",\n",
      "   \"document_used\": [\n",
      "      2,\n",
      "      3,\n",
      "      5\n",
      "   ],\n",
      "   \"answer\": \"The study uses machine learning to identify baseline and time-varying factors that predict the likelihood of edema in patients treated with tepotinib. It assesses 54 covariates and employs explainability tools like SHAP values to analyze the relationship between these factors and edema outcomes.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{'chunk_id': '61', 'chunk': Chunk(chunk_id=61, content='<span id=\"page-11-1\"></span>Additional supporting information can be found online in the Supporting Information section at the end of this article. **How to cite this article:** Amato F, Strotmann R, Castello R, et al. Explainable machine learning prediction of edema adverse events in patients treated with tepotinib. *Clin Transl Sci*. 2024;17:e70010. doi[:10.1111/cts.70010](https://doi.org/10.1111/cts.70010)', metadata={'source_text': 'Explainable_machine_learning_prediction_of_edema_a.pdf', 'document_chunk_id': 61}, data_type=<DataType.TEXT: 'text'>, score=None), 'fused_score': 0.016129032258064516, 'average_original_score': 1.6858880519866943}\n"
     ]
    }
   ],
   "source": [
    "# The documents retrieved by the retriever:\n",
    "print(len(sources))\n",
    "print(sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003126\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quickly go through the code and the notebook above to ensure you understand how each block works.\n",
    "\n",
    "No solution provided for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Questions about `Explainable_machine_learning_prediction_of_edema_a.pdf`\n",
    "\n",
    "Answer the following questions related to `Explainable_machine_learning_prediction_of_edema_a.pdf` and analyze the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 \"What was identified as the most important predictor for edema occurrence?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant, and your task is to answer questions using relevant documents. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\n",
      "\n",
      "Here are the relevant DOCUMENTS:\n",
      "\n",
      "\n",
      "Document 1: \n",
      "Consistently with the above sensitivity analysis, past current edema grade was found to be the most influential input, particularly if a same grade persisted to the following safety visit. The exposure-derived features were also informative for the model probability predictions. Albumin was found as the most informative time-varying covariate, especially for predicting edemas of grades 2+. Figure [3](#page-7-1) illustrates the contribution of the input variables toward the predicted probability of edemas of grades 2+. The analysis reveals that the current edema grade is the most informative input, as patients with a history of edemas of grades 2+ are considered highly likely to experience the same grade in the future. Interestingly, albumin once again emerges as the most informative among the longitudinal covariates, with lower levels associated <span id=\"page-6-0\"></span>\n",
      "\n",
      "Document 2: \n",
      "Advanced age was also found as predictive of edemas of grade 2+, in agreement with current knowledge of the investigated adverse event behavior, for which age is known to be a risk factor independently from drug exposure.[36,37](#page-11-11) Finally, the time until the next visit, used to inform the model about the forecasting horizon, was also informative. However, this input should be seen as a factor reflecting the deteriorating status of patients, as changes in medical condition could prompt clinicians to schedule short-term (re-)assessment visits. One of the primary challenges encountered in this study's classification setting was the unbalanced representation of different edema grades in the data. To overcome this issue, the model was set up to produce a probability for each edema grade of any new patient instance. When dealing with unbalanced data, such probabilities can be small, which is not a problem as long as they are accurate.\n",
      "\n",
      "Document 3: \n",
      "The presence of such input is ensuring high model performances when predicting persistence of a given edema grade. The sensitivity analysis to the inclusion of this input revealed a decline in the mean cross-validation F1 score of ~0.350 when it was removed as a candidate predictor. However, the use of Isotonic Regressions ensures the estimations of correctly calibrated probabilities. For instance, while low predicted probabilities, for example, for edemas of grade 2+, might lead to a classification error, they can still provide valuable details on existing risk of occurrence of an adverse event (Figure [S5\\)](#page-11-0). Moreover, sensitivity analysis together with the SHAP importance highlighted interesting patterns with respect to the exposure-related features. Referencing Figure [3](#page-7-1), lower values of the dose [ *t* − 14 days, *t* ] are associated with a decreased probability of edema of grades 2+ at subsequent visits.\n",
      "\n",
      "Document 4: \n",
      "Edema is known to be the most prevalent adverse event, and a known class effect of MET inhibitors[17,18,31](#page-10-9) indicating that the underlying pathology may be related to a target-mediated effect[.10](#page-10-6) The MET ligand, HGF, and the MET/PI3k/Akt pathway could play a role in regulating endothelial permeability[.32,33](#page-11-10) Blocking the HGF/MET signaling axis may result in compromised endothelial barrier integrity, leading to fluid retention and edema. A wider understanding of the relationships between clinical covariates and the occurrence of edema is of practical importance in the evaluation and management of risk for treatment-emergent edema during pharmacotherapy with MET inhibitors. This study was aimed at formalizing a framework to apply ML algorithms on longitudinal clinical data and testing it on the case of edema occurrence in patients from five Phase I/II clinical studies receiving tepotinib monotherapy at doses of 30–1400mg, once daily.\n",
      "\n",
      "Document 5: \n",
      "The second objective of the study was the identification of the factors predicting edema occurrence and evolution over time. The Shapley Additive exPlanations (SHAP) method was used to investigate the role different factors have toward a specific estimation of edema occurrence obtained via the best predictive model, both at population and patient level. The use of this approach overcomes the lack of explainability of ML models, which approximate complex nonlinear functions from data in a not straightforwardly interpretable manner.[20,21](#page-10-11) # **METHODS** # **Clinical data** Data from 612 patients enrolled in five Phase I/II clinical studies with tepotinib were collected (NCT01014936, NCT01832506, NCT01988493, NCT02115373, VISION – NCT02864992).\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Here is the USER QUESTION:\n",
      "What was identified as the most important predictor for edema occurrence?\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Please think step-by-step and generate your output in json:\n",
      "\n",
      "Total input tokens: 1120\n",
      "Total output tokens: 148\n",
      "Total tokens: 1268\n",
      "Estimated cost: $0.0003\n"
     ]
    }
   ],
   "source": [
    "answer, sources, cost = rag.execute(\n",
    "    \"What was identified as the most important predictor for edema occurrence?\",\n",
    "    {},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"step_by_step_thinking\": \"To answer the question about the most important predictor for edema occurrence, I reviewed Document 1, which highlighted that the past current edema grade was found to be the most influential input. Additionally, it mentioned that albumin was notably informative as a time-varying covariate. However, the specific mention of the 'current edema grade' as the most informative input indicates that this is the primary focus when predicting future occurrences of edema grades 2+. Therefore, the current edema grade stands out as the most critical predictor of edema occurrence.\",\n",
      "   \"document_used\": [\n",
      "      1\n",
      "   ],\n",
      "   \"answer\": \"The past current edema grade was identified as the most important predictor for edema occurrence.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{'chunk_id': '32', 'chunk': Chunk(chunk_id=32, content='Consistently with the above sensitivity analysis, past current edema grade was found to be the most influential input, particularly if a same grade persisted to the following safety visit. The exposure-derived features were also informative for the model probability predictions. Albumin was found as the most informative time-varying covariate, especially for predicting edemas of grades 2+. Figure [3](#page-7-1) illustrates the contribution of the input variables toward the predicted probability of edemas of grades 2+. The analysis reveals that the current edema grade is the most informative input, as patients with a history of edemas of grades 2+ are considered highly likely to experience the same grade in the future. Interestingly, albumin once again emerges as the most informative among the longitudinal covariates, with lower levels associated <span id=\"page-6-0\"></span>', metadata={'source_text': 'Explainable_machine_learning_prediction_of_edema_a.pdf', 'document_chunk_id': 32}, data_type=<DataType.TEXT: 'text'>, score=None), 'fused_score': 0.016129032258064516, 'average_original_score': 0.8272292613983154}\n"
     ]
    }
   ],
   "source": [
    "# The documents retrieved by the retriever:\n",
    "print(len(sources))\n",
    "print(sources[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 \"Which machine learning algorithm performed best for predicting edema, and what was its F1 score?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant, and your task is to answer questions using relevant documents. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\n",
      "\n",
      "Here are the relevant DOCUMENTS:\n",
      "\n",
      "\n",
      "Document 1: \n",
      "Generalization performances were assessed on the previously unused test set, resulting in a weighted F1 score of 0.959. Effects of calibration via Isotonic Regression on such model are shown in Figures [S3,](#page-11-0) [S4.](#page-11-0) Then, SHAP values were used to determine the 10 most relevant predictors for this model. Finally, a last model was trained using only such predictors, leading to a weighted F1 score of 0.961. Precision and recall values for this model are reported in Table [S2](#page-11-0), showing consistent results across the different output classes. As increased age was previously found to be associated with increasing risk of edema,[10](#page-10-6) the performances of the model have been verified within the different age terciles, showing consistent results across them (weighted F1 score equal to 0.969, 0.975, and 0.938 for the three terciles, respectively).\n",
      "\n",
      "Document 2: \n",
      "To determine the best algorithm and covariate engineering approach to predict edema occurrence and grade, the five different input spaces obtained after covariate engineering were used as input to both RF and GBT, yielding a total of 10 different model settings. Only data up to February 2021 and without VISION Cohort C patients were used to generate the train and the test set. After model training, performances of the different settings have been assessed via the mean cross-validation error obtained over the fivefold for the best combination of hyperparameters for each model. Table [2](#page-6-0) shows the mean F1 score and the corresponding standard deviation for all the settings. Although RF performs better than GBT for the specific task of edema prediction, there were no significant differences in the F1 score across the models trained with covariates resulting from different engineering approaches.\n",
      "\n",
      "Document 3: \n",
      "All models were trained using stratified grouped k-fold cross-validation.[26](#page-10-17) To account for the imbalance of the edema grades in the classification target, precision, recall, and weighted and macro F1 scores were used as quantitative metrics to evaluate model performances.[27](#page-11-6) Probability calibration via Isotonic Regression was used to ensure that confidence scores predicted by the classifier – in this case, one of the RF or GBT models – were matching the true empirical frequencies of edema grades.[28](#page-11-7) Practically, the cross-validation procedure previously described was used to obtain unbiased predictions for all the data. Then, the unbiased predictions within each fold were used to train the Isotonic Regression. Further details on probability calibration and Isotonic Regression are provided in Supplementary Materials [S1.](#page-11-0) The last step of the methodology deals with model explainability.\n",
      "\n",
      "Document 4: \n",
      "Therefore, the RF model that utilized the multiple visit approach for time-varying covariates was deemed the best compromise between model performance, longitudinal data exploitation, and easiness of result interpretation. The performances of the selected model were further assessed on the VISION Cohort A data collected from February 2021 to November 2022 and on data from VISION Cohort C. No changes were observed in evaluation metrics when the model was used for predicting edema occurrence and grade in the patients from Cohort C, with the weighted F1 score estimated as 0.944. The F1 score reached 0.994 for follow-up data from Cohort A. Such an increase in the metric is mostly to be attributed to the fact that stable edema conditions were assessed for 74 of the 94 patients, for whom follow-up data were available. # **Final model on extended dataset** The RF model using the actual value at the visit for timevarying covariates was retrained using data from all available patients.\n",
      "\n",
      "Document 5: \n",
      "Data from 612 patients receiving tepotinib in five Phase I/II studies were modeled with two ML algorithms, Random Forest, and Gradient Boosting Trees, to predict edema AE incidence and severity. Probability calibration was applied to give a realistic estimation of the likelihood of edema AE. Best model was tested on follow-up data and on data from clinical studies unused while training. Results showed high performances across all the tested settings, with F1 scores up to 0.961 when retraining the model with the most relevant covariates. The use of ML explainability methods identified serum albumin as the most informative longitudinal covariate, and higher age as associated with higher probabilities of more severe edema. The developed methodological framework enables the use of ML algorithms for analyzing clinical safety data and exploiting longitudinal information through various covariate engineering approaches.\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Here is the USER QUESTION:\n",
      "Which machine learning algorithm performed best for predicting edema, and what was its F1 score?\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Please think step-by-step and generate your output in json:\n",
      "\n",
      "Total input tokens: 1138\n",
      "Total output tokens: 176\n",
      "Total tokens: 1314\n",
      "Estimated cost: $0.0003\n"
     ]
    }
   ],
   "source": [
    "answer, sources, cost = rag.execute(\n",
    "    \"Which machine learning algorithm performed best for predicting edema, and what was its F1 score?\",\n",
    "    {},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"step_by_step_thinking\": \"I analyzed the documents to find information on the performance of machine learning algorithms used for predicting edema. Document 2 mentions that the Random Forest (RF) algorithm performs better than Gradient Boosting Trees (GBT) for edema prediction. Document 1 provides specific F1 scores, noting an F1 score of 0.961 for models trained with relevant predictors. Additionally, Document 4 indicates that the RF model achieved an F1 score of 0.994 during evaluations on follow-up data. This suggests that the best algorithm for predicting edema is Random Forest, and its highest reported F1 score is 0.994.\",\n",
      "   \"document_used\": [\n",
      "      2,\n",
      "      1,\n",
      "      4\n",
      "   ],\n",
      "   \"answer\": \"The Random Forest algorithm performed best for predicting edema, with an F1 score of 0.994.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{'chunk_id': '29', 'chunk': Chunk(chunk_id=29, content='Generalization performances were assessed on the previously unused test set, resulting in a weighted F1 score of 0.959. Effects of calibration via Isotonic Regression on such model are shown in Figures [S3,](#page-11-0) [S4.](#page-11-0) Then, SHAP values were used to determine the 10 most relevant predictors for this model. Finally, a last model was trained using only such predictors, leading to a weighted F1 score of 0.961. Precision and recall values for this model are reported in Table [S2](#page-11-0), showing consistent results across the different output classes. As increased age was previously found to be associated with increasing risk of edema,[10](#page-10-6) the performances of the model have been verified within the different age terciles, showing consistent results across them (weighted F1 score equal to 0.969, 0.975, and 0.938 for the three terciles, respectively).', metadata={'source_text': 'Explainable_machine_learning_prediction_of_edema_a.pdf', 'document_chunk_id': 29}, data_type=<DataType.TEXT: 'text'>, score=None), 'fused_score': 0.016129032258064516, 'average_original_score': 0.5976967811584473}\n"
     ]
    }
   ],
   "source": [
    "# The documents retrieved by the retriever:\n",
    "print(len(sources))\n",
    "print(sources[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 \"How did cumulative tepotinib dose impact edema predictions, and what insights did SHAP provide about this relationship?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant, and your task is to answer questions using relevant documents. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\n",
      "\n",
      "Here are the relevant DOCUMENTS:\n",
      "\n",
      "\n",
      "Document 1: \n",
      "The second objective of the study was the identification of the factors predicting edema occurrence and evolution over time. The Shapley Additive exPlanations (SHAP) method was used to investigate the role different factors have toward a specific estimation of edema occurrence obtained via the best predictive model, both at population and patient level. The use of this approach overcomes the lack of explainability of ML models, which approximate complex nonlinear functions from data in a not straightforwardly interpretable manner.[20,21](#page-10-11) # **METHODS** # **Clinical data** Data from 612 patients enrolled in five Phase I/II clinical studies with tepotinib were collected (NCT01014936, NCT01832506, NCT01988493, NCT02115373, VISION – NCT02864992).\n",
      "\n",
      "Document 2: \n",
      "The combination of the two results suggests that other covariate(s) included in the model might act as surrogate(s) of the exposure, informing the model about its role even in the case in which dose-related variables are excluded from its input. This might, for example, be true for serum albumin as treatment-emergent hypoalbuminemia was already reported for several MET inhibitors, including tepotinib, and a relationship to tepotinib plasma concentrations has been previously described.[10,34,35](#page-10-6) Furthermore, SHAP analysis revealed an association between lower levels of albumin and an increased predicted probability of edemas of grades 2+. This is consistent with previous findings which highlighted a trend indicating a positive relationship between the magnitude of decrease in serum albumin and the maximum severity of edema.[10](#page-10-6) Indeed, albumin has a physiological role of maintaining oncotic pressure, hence is potentially a factor for edema pathogenesis.\n",
      "\n",
      "Document 3: \n",
      "*Clinical and Translational Science* published by Wiley Periodicals LLC on behalf of American Society for Clinical Pharmacology and Therapeutics. dose modifications in reducing this AE, exploring its relationship with potential prognostic factors. ### **WHAT QUESTION DID THIS STUDY ADDRESS?** Are there baseline and time-varying factors to support the identification of higher likelihood of edema occurrence in patients receiving tepotinib treatment? ### **WHAT DOES THIS STUDY ADD TO OUR KNOWLEDGE?** This study assesses 54 covariates as predictors of edema using ML. Explainability tools investigate the relationship between input covariates and predicted outcomes. The identified drivers align with the existing knowledge of the investigated AE behavior. ### **HOW MIGHT THIS CHANGE DRUG DISCOVERY, DEVELOPMENT, AND/OR THERAPEUTICS?**\n",
      "\n",
      "Document 4: \n",
      "As a decrease in such covariate would be a consequence of a dose reduction or of a temporary treatment interruption, this suggest that the latter are an effective approach to mitigate edema. Conversely, dose [ *t*0, *t* − 15 days] is influenced by the fact that the longer a patient remains on treatment, the higher the cumulative long-term dose. Figure [3](#page-7-1) indicates that patients with higher cumulative doses have a lower probability of experiencing edema of grade 2+, suggesting that these patients are undergoing a longer duration of treatment without severe adverse effects. The exclusion of the exposure-related features seems to affect the model classification performances only marginally, as shown in Table [3](#page-6-1). However, they were identified as among the most important inputs in the SHAP analyses.\n",
      "\n",
      "Document 5: \n",
      "Data from 612 patients receiving tepotinib in five Phase I/II studies were modeled with two ML algorithms, Random Forest, and Gradient Boosting Trees, to predict edema AE incidence and severity. Probability calibration was applied to give a realistic estimation of the likelihood of edema AE. Best model was tested on follow-up data and on data from clinical studies unused while training. Results showed high performances across all the tested settings, with F1 scores up to 0.961 when retraining the model with the most relevant covariates. The use of ML explainability methods identified serum albumin as the most informative longitudinal covariate, and higher age as associated with higher probabilities of more severe edema. The developed methodological framework enables the use of ML algorithms for analyzing clinical safety data and exploiting longitudinal information through various covariate engineering approaches.\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Here is the USER QUESTION:\n",
      "How did cumulative tepotinib dose impact edema predictions, and what insights did SHAP provide about this relationship?\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Please think step-by-step and generate your output in json:\n",
      "\n",
      "Total input tokens: 1060\n",
      "Total output tokens: 211\n",
      "Total tokens: 1271\n",
      "Estimated cost: $0.0003\n"
     ]
    }
   ],
   "source": [
    "answer, sources, cost = rag.execute(\n",
    "    \"How did cumulative tepotinib dose impact edema predictions, and what insights did SHAP provide about this relationship?\",\n",
    "    {},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"step_by_step_thinking\": \"I examined Document 4, where it is noted that a higher cumulative tepotinib dose correlates with a lower probability of experiencing severe edema (grade 2+). This suggests that patients receiving longer treatment durations experience fewer severe adverse effects due to cumulative dosing. Additionally, Document 5 discusses the use of SHAP analysis, which revealed that certain covariates, including age and serum albumin levels, significantly influence edema predictions. Together, these documents indicate that while higher cumulative doses tend to reduce severe edema occurrences, SHAP provided insightful relationships that inform how various factors, including treatment duration and specific patient characteristics, interact with edema risk.\",\n",
      "   \"document_used\": [\n",
      "      4,\n",
      "      5\n",
      "   ],\n",
      "   \"answer\": \"Higher cumulative tepotinib dose is associated with a lower probability of severe edema (grade 2+). SHAP analysis identified that serum albumin levels and patient age are significant factors influencing edema predictions, showing how these variables interact with treatment duration and severity of adverse effects.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{'chunk_id': '11', 'chunk': Chunk(chunk_id=11, content='The second objective of the study was the identification of the factors predicting edema occurrence and evolution over time. The Shapley Additive exPlanations (SHAP) method was used to investigate the role different factors have toward a specific estimation of edema occurrence obtained via the best predictive model, both at population and patient level. The use of this approach overcomes the lack of explainability of ML models, which approximate complex nonlinear functions from data in a not straightforwardly interpretable manner.[20,21](#page-10-11) # **METHODS** # **Clinical data** Data from 612 patients enrolled in five Phase I/II clinical studies with tepotinib were collected (NCT01014936, NCT01832506, NCT01988493, NCT02115373, VISION – NCT02864992).', metadata={'source_text': 'Explainable_machine_learning_prediction_of_edema_a.pdf', 'document_chunk_id': 11}, data_type=<DataType.TEXT: 'text'>, score=None), 'fused_score': 0.016129032258064516, 'average_original_score': 0.5643692016601562}\n"
     ]
    }
   ],
   "source": [
    "# The documents retrieved by the retriever:\n",
    "print(len(sources))\n",
    "print(sources[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Propose your own question.\n",
    "\n",
    "No solution provided for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Review the `Modeling tumor size dynamics based on real‐world electronic health records.pdf` and come up with a question. Ask it and analyze the answer, confirm that the retriever uses relevant chunks from this source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant, and your task is to answer questions using relevant documents. Please first think step-by-step by mentioning which documents you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"document_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\n",
      "\n",
      "Here are the relevant DOCUMENTS:\n",
      "\n",
      "\n",
      "Document 1: \n",
      "The paucity of baseline images prevented us to estimate the growth rate constant that was fixed to a literature value obtained from a population similar to our cohort[.4](#page-10-3) Sensitivity analyses provided confidence regarding the suitability of the value for our analysis. Our data supported an ON/OFF treatment effect, with better model performances as compared to a model with a treatment effect linearly dependent on the doses. This is in line with literature evidence indicating that, at the dose ranges used in clinical practice, the exposure-response relationship of ICIs is at the plateau of the maximal response.[4,32](#page-10-3) Additionally, we investigated different combinations of *k*kill to deal with limited data in our population of patients ![](_page_8_Figure_1.jpeg) <span id=\"page-8-0\"></span>**FIGURE 2** Forest plots of covariate effects on TTB0 for the model including clinical covariates.\n",
      "\n",
      "Document 2: \n",
      "Different hypotheses on treatment dynamics were compared: ON/OFF treatment effect (using a 1/0 regressor in the dataset), treatment effect linearly dependent on drug dose, or kinetic-pharmacodynamic model[20](#page-10-13) using linear relationship with dose. Images collected after treatment discontinuation (and prior to initiation of a subsequent treatment line) were included, and the effect of ICIs discontinuation assessed. ## Parameter estimation and model selection Tumor measurements were fitted using the Stochastic Approximation Expectation–Maximization algorithm ![](_page_4_Figure_1.jpeg) <span id=\"page-4-0\"></span>**FIGURE 1** Multistep machine-learning-based selection of radiomics features as covariate candidates. Before feature selection, we first established whether the radiomic dataset carries any relevant information for predicting each of the model parameters (step 0).\n",
      "\n",
      "Document 3: \n",
      "A significantly lower OFV was observed in the model with an ON/OFF treatment effect compared to the one with a treatment effect linearly dependent on drug dose (∆OFV=−10.3, *p*<0.05). The estimation of a single shared killing rate constant for the three drugs decreased BIC (∆BIC=−16.0 compared to a model with three killing rate constants and the associated IIV) and increased precision of parameter estimates. The same model parameters were used for modeling tumor dynamics during treatment and after ICIs discontinuation. Estimating separate residual errors for each image modality (i.e., CT or PET/CT) failed to improve the model. IIVs were estimated on TTB0 and *k*kill. Despite a statistically nonsignificant decrease in OFV when adding IIV on *k*growth (∆OFV=−2.97, *p*=0.09), it was retained in the base model because it makes sense that not all tumors grow at the same pace and to allow further covariate assessment on this parameter.\n",
      "\n",
      "Document 4: \n",
      "A data-driven stepwise procedure was used to find the model that adequately fitted the data (Supplementary Material [S1](#page-11-0)). Briefly, each model included at least a baseline tumor size parameter before treatment initiation (TTB0 in cm3 ), a growth rate constant (*k*growth in days−1) and a killing rate constant (*k*kill in days−1). When baseline observation was missing, we kept the time zero observation with missing dependent value to further allow its empirical Bayes estimate (EBE). An exponential tumor growth with a fixed *k*growth retrieved from literature in a population of melanoma patients receiving pembrolizumab[4](#page-10-3) was assumed (Supplementary Material [S1\\)](#page-11-0). Models with a dedicated or shared *k*kill for all drugs or for the two PD-1 inhibitors were compared. Drug effects were included in an additive form.\n",
      "\n",
      "Document 5: \n",
      "As a decrease in such covariate would be a consequence of a dose reduction or of a temporary treatment interruption, this suggest that the latter are an effective approach to mitigate edema. Conversely, dose [ *t*0, *t* − 15 days] is influenced by the fact that the longer a patient remains on treatment, the higher the cumulative long-term dose. Figure [3](#page-7-1) indicates that patients with higher cumulative doses have a lower probability of experiencing edema of grade 2+, suggesting that these patients are undergoing a longer duration of treatment without severe adverse effects. The exclusion of the exposure-related features seems to affect the model classification performances only marginally, as shown in Table [3](#page-6-1). However, they were identified as among the most important inputs in the SHAP analyses.\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Here is the USER QUESTION:\n",
      "What was the rationale for using an ON/OFF treatment effect model instead of a dose-dependent model?\n",
      "\n",
      "--------------------------------------------\n",
      "\n",
      "Please think step-by-step and generate your output in json:\n",
      "\n",
      "Total input tokens: 1160\n",
      "Total output tokens: 216\n",
      "Total tokens: 1376\n",
      "Estimated cost: $0.0003\n"
     ]
    }
   ],
   "source": [
    "answer, sources, cost = rag.execute(\n",
    "    \"What was the rationale for using an ON/OFF treatment effect model instead of a dose-dependent model?\",\n",
    "    {},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"step_by_step_thinking\": \"To understand the rationale for using an ON/OFF treatment effect model over a dose-dependent model, I analyzed several documents. Document 1 states that the data supported an ON/OFF treatment effect with better model performances as compared to a model with a treatment effect linearly dependent on the doses. Document 3 mentions a significantly lower objective function value (OFV) in the ON/OFF treatment effect model compared to the dose-dependent model, indicating that the ON/OFF model is a better fit for the data. Together, these documents suggest that the ON/OFF treatment effect provides a more accurate representation of how treatment may affect outcomes in this patient population, especially at the dose levels used in clinical practice.\",\n",
      "   \"document_used\": [\n",
      "      1,\n",
      "      3\n",
      "   ],\n",
      "   \"answer\": \"The rationale for using an ON/OFF treatment effect model is based on its better model performance and lower objective function value (OFV) compared to a dose-dependent model, indicating it fits the data better and captures the treatment dynamics more effectively.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{'chunk_id': '103', 'chunk': Chunk(chunk_id=103, content='The paucity of baseline images prevented us to estimate the growth rate constant that was fixed to a literature value obtained from a population similar to our cohort[.4](#page-10-3) Sensitivity analyses provided confidence regarding the suitability of the value for our analysis. Our data supported an ON/OFF treatment effect, with better model performances as compared to a model with a treatment effect linearly dependent on the doses. This is in line with literature evidence indicating that, at the dose ranges used in clinical practice, the exposure-response relationship of ICIs is at the plateau of the maximal response.[4,32](#page-10-3) Additionally, we investigated different combinations of *k*kill to deal with limited data in our population of patients ![](_page_8_Figure_1.jpeg) <span id=\"page-8-0\"></span>**FIGURE 2** Forest plots of covariate effects on TTB0 for the model including clinical covariates.', metadata={'source_text': 'Modeling tumor size dynamics based on real‐world electronic health records.pdf', 'document_chunk_id': 41}, data_type=<DataType.TEXT: 'text'>, score=None), 'fused_score': 0.016129032258064516, 'average_original_score': 0.8026544451713562}\n"
     ]
    }
   ],
   "source": [
    "# The documents retrieved by the retriever:\n",
    "print(len(sources))\n",
    "print(sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source_text': 'Modeling tumor size dynamics based on real‐world electronic health records.pdf', 'document_chunk_id': 41}\n",
      "{'source_text': 'Modeling tumor size dynamics based on real‐world electronic health records.pdf', 'document_chunk_id': 19}\n",
      "{'source_text': 'Modeling tumor size dynamics based on real‐world electronic health records.pdf', 'document_chunk_id': 27}\n",
      "{'source_text': 'Modeling tumor size dynamics based on real‐world electronic health records.pdf', 'document_chunk_id': 18}\n",
      "{'source_text': 'Explainable_machine_learning_prediction_of_edema_a.pdf', 'document_chunk_id': 43}\n"
     ]
    }
   ],
   "source": [
    "for source in sources:\n",
    "    print(source[\"chunk\"].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four most relevant sources come indeed from the correct document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discuss how the pipeline could be improved to achieve better answers. If time permits, implement those changes.\n",
    "\n",
    "No solution provided for this exercise.\n",
    "\n",
    "Participants can discuss every step of the pipeline (data extraction, embedding models, retrieval improvement, sending more chunks, providing to the LLM part of the metadata (which document does the chunk come from), better prompting, ect.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-llm-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
