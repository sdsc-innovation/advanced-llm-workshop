{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Multi-Modal RAG Hands-On Theory Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What is RAG?\n",
    "\n",
    "![RAG Image](images/rag.png)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is an AI technique that combines information retrieval with text generation. Instead of relying solely on a pre-trained language model’s internal knowledge, RAG dynamically retrieves relevant documents from an external knowledge base before generating a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Why Rag?\n",
    "\n",
    "![Why RAG Image](images/why_rag.png)\n",
    "\n",
    "1. **Improved Accuracy:** RAG enhances the factual correctness of generated responses by retrieving up-to-date and domain-specific information, reducing the likelihood of hallucinations (fabricated information).\n",
    "\n",
    "2. **Better Generalization:** Since RAG dynamically retrieves relevant documents, it performs well across various domains without requiring extensive fine-tuning, making it more adaptable to new topics.\n",
    "\n",
    "3. **Reduced Model Size Requirements:** Instead of embedding all knowledge within a large model, RAG leverages external databases, allowing for smaller, more efficient models while maintaining high-quality responses.\n",
    "\n",
    "4. **Enhanced Explainability:** By referencing retrieved documents, RAG provides verifiable sources for its answers, making it more transparent and easier to trust compared to purely generative models.\n",
    "\n",
    "5. **And more...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What is Multi-modal RAG?\n",
    "\n",
    "Multi-modal Retrieval-Augmented Generation (RAG) extends the standard RAG approach by incorporating **multiple types of data**—such as text, images, or even audio—**into the retrieval and generation process**. Instead of working with a single modality (like just text), multi-modal RAG systems can query and generate content based on various forms of input, allowing for richer and more diverse responses. \n",
    "\n",
    "For example, when dealing with a document that contains both images and text, a multi-modal RAG system can retrieve relevant images along with the associated text, enhancing the quality and relevance of the generated response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Text+Images RAG\n",
    "\n",
    "Multi-modal RAG (Retrieval-Augmented Generation) systems differ in how they handle text and images, depending on whether both the database and the language model (LLM) work with text and images together or focus on one modality. There are a lot of different approaches to reach this goal.\n",
    "\n",
    "![Multi-Modal RAG Image](images/multi_modal_rag.jpg)\n",
    "\n",
    "Another approach not shown above is to consider the file as a series of images:\n",
    "- **File-as-Images** → **Image DB** → **Retrieve Images** → **Multi-modal LLM** → **Text answer and sources**  \n",
    "- In this case, the document is converted into a series of images (e.g., scanned pages), stored in an image-specific database, and sent as images to a multi-modal LLM, which generates textual answers based on the content of the images. The DB used is usually specialized for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Hands-on Example\n",
    "\n",
    "In this exercise, you will learn how to implement a **Multimodal Retrieval-Augmented Generation (RAG)** pipeline from scratch, without relying on tools like `langchain`.  While `langchain` is a powerful framework that simplifies the development of RAG pipelines, it can sometimes lack flexibility for custom implementations, as it abstracts many components.\n",
    "\n",
    "Here, two different vector store are used to **store the images and text separately**.\n",
    "\n",
    "The different components of the pipeline are:\n",
    "\n",
    "- **Text and image extraction from PDFs** – Extract raw text and images from PDF files to make the content processable.  \n",
    "- **Text and image chunking** – Break the extracted text and images into smaller, meaningful segments to improve retrieval efficiency.  \n",
    "- **Embedding of the chunks (text and images)** – Convert text and image chunks into numerical representations (embeddings) using pre-trained models.  \n",
    "- **Storage of the embeddings in a vector store** – Save both text and image embeddings in a specialized database (vector store) to enable fast similarity searches.  \n",
    "- **Relevant chunks retrieval** – Query the vector store to find the most relevant text and image chunks based on user input.  \n",
    "- **Setting and prompting of the LLM for a RAG** – Structure prompts and configure the language model to integrate retrieved text and image information into its responses.  \n",
    "- **Additional tools for improved retrieval** – Use techniques like query expansion to reformulate user queries for better recall and reciprocal rank fusion to combine results from multiple retrieval methods.  \n",
    "- **Final multimodal RAG pipeline implementation** – Integrate all components into a complete system that retrieves relevant information (both text and images) and generates enhanced responses using the language model.\n",
    "\n",
    "**Note:** To complete this exercise, you need an OpenAI API key, the PDF files with images, and the necessary libraries installed (see `requirements.txt`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The example is applied to `Explainable_machine_learning_prediction_of_edema_a.pdf`. Please, have a quick look at it before starting the exercise.\n",
    "\n",
    "We will try to answer the following question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_question = \"According to SHAP analysis, which factors were the most influential in predicting higher-grade edema (Grade 2+)?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import getpass\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from helpers.constants_and_data_classes import Chunk, DataType\n",
    "from helpers.data_processing import PDFExtractor, SimpleChunker\n",
    "from helpers.embedding import (\n",
    "    OpenAITextEmbeddings,\n",
    "    OpenAITextEmbeddingsAzure,\n",
    "    ImageEmbeddings,\n",
    "    ImageEmbeddingsForText,\n",
    "    compute_openai_large_embedding_cost,\n",
    ")\n",
    "from helpers.vectorstore import (\n",
    "    ChromaDBVectorStore,\n",
    "    VectorStoreRetriever,\n",
    ")\n",
    "from helpers.llm import OpenAILLM, OpenAILLMAzure\n",
    "from helpers.rag import Generator, DefaultRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_folder = \"../../data\"\n",
    "\n",
    "example_pdf_file = \"Explainable_machine_learning_prediction_of_edema_a.pdf\"\n",
    "example_pdf_path = os.path.join(data_folder, example_pdf_file)\n",
    "\n",
    "text_vector_store_collection = \"text_collection\"\n",
    "image_vector_store_collection = \"image_collection\"\n",
    "\n",
    "text_vector_store_full_collection = \"text_collection_full\"\n",
    "image_vector_store_full_collection = \"image_collection_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "## If Azure Endpoint then you don't need the OPENAI_API_KEY but the following\n",
    "# os.environ[\"AZURE_API_KEY\"] = \"\"\n",
    "# os.environ[\"AZURE_API_BASE\"] = \"\"\n",
    "# os.environ[\"AZURE_API_VERSION\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## LLM  \n",
    "\n",
    "The LLM is the core of the RAG system, responsible for generating responses based on the retrieved information. There are many options available on-premise or online, each with different performance, speed, specialized knowledge and cost trade-offs.\n",
    "\n",
    "In this case a **multi-modal LLM is required**, we use `gpt-4o-mini`.  \n",
    "\n",
    "This LLM expects input in the form of a list of messages, where each message includes the content and the role of the speaker (e.g., developer, user, assistant).  \n",
    "\n",
    "Images can be provided to this LLM as `base64`, but only when the role is set to `user`.\n",
    "\n",
    "Here is how messages are defined here:\n",
    "\n",
    "```python\n",
    "class Roles(str, Enum):\n",
    "    DEVELOPER = \"developer\" # Previously, system\n",
    "    USER = \"user\"\n",
    "    ASSISTANT = \"assistant\"\n",
    "    TOOL = \"tool\"\n",
    "\n",
    "class LLMMessage(BaseModel):\n",
    "    content: Optional[str] = None\n",
    "    role: Optional[Roles] = None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if both Azure environment variables exist\n",
    "azure_endpoint = os.getenv(\"AZURE_API_BASE\")\n",
    "azure_api_key = os.getenv(\"AZURE_API_KEY\")\n",
    "if azure_endpoint and azure_api_key:\n",
    "    llm = OpenAILLMAzure(temperature=0.5)\n",
    "    print(\"Using AzureOpenAI client\")\n",
    "else:\n",
    "    llm = OpenAILLM(temperature=0.5)\n",
    "    print(\"Using OpenAI client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, price = llm.generate([{\"role\": \"user\", \"content\": test_question}], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## PDF Text and Images Extraction  \n",
    "\n",
    "The first step in the pipeline is to extract text and images from the document.  \n",
    "\n",
    "In this exercise, we use the `MinerU` library, which under the hood uses among others `doclayout_yolo` for segmentation. Note that this model is not commercially permissive.\n",
    "\n",
    "The choice of extraction tool should be carefully considered. Depending on the document type and formatting, different methods may be required to preserve text integrity and leverage structural elements such as headings, tables, or metadata for better processing (`pdfplumber` (better for tables), `Tesseract OCR` (for scanned PDFs), ect.).\n",
    "\n",
    "Extracting images can be challenging, as **irrelevant images** (such as logos) are often included, and some images may be **split into multiple images**. It may also be helpful to link the position of images to nearby text for more accurate retrieval. Specialized tools or methods might be required to efficiently handle images embedded in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_extractor = PDFExtractor()\n",
    "_, text, images = data_extractor.extract_text_and_images(example_pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_data = base64.b64decode(images[2][\"image_base64\"])\n",
    "img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_data = base64.b64decode(images[0][\"image_base64\"])\n",
    "img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Chunking\n",
    "\n",
    "The second step is to split the extracted text into smaller chunks, which will later be embedded and retrieved efficiently. \n",
    "\n",
    "In this exercise, we use a simple heuristic approach: the text is split iteratively—first by heading levels (`#`), then by line breaks (`\\n`), and finally by sentence (`.`). Splitting only occurs if the resulting chunk exceeds a predefined length. However, more advanced techniques exist, such as **semantic chunking** (which splits based on meaning rather than syntax) or **agentic chunking** (which dynamically adapts chunk sizes based on context).\n",
    "\n",
    "**Images are treated as separate chunks**, but with a different `DataType`. Additional relevant metadata can also be included, such as the image's position relative to the text or its caption, if available. They are stored in another list.\n",
    "\n",
    "Each chunk is enriched with metadata, including:  \n",
    "- **Source file** – The document from which the chunk originates.  \n",
    "- **Chunk counter** – The position of the chunk within the file.  \n",
    "- **Unique identifier (`chunk_id`)** – Ensures each chunk can be referenced independently.  \n",
    "- **Data type** - The document type (image or text).\n",
    "\n",
    "Additional metadata could be included to enable more refined filtering and retrieval strategies.  \n",
    "\n",
    "```python\n",
    "class DataType(str, Enum):\n",
    "    TEXT = \"text\"\n",
    "    IMAGE = \"image\"\n",
    "\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    chunk_id: int\n",
    "    content: str\n",
    "    metadata: dict = Field(default_factory=dict)\n",
    "    data_type: Optional[DataType] = None\n",
    "    score: Optional[float] = None\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunker = SimpleChunker()\n",
    "text_chunks = chunker.chunk_text(text, {\"source_text\": example_pdf_file})\n",
    "image_chunks = chunker.chunk_images(images, {\"source_text\": example_pdf_file})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(text_chunks))\n",
    "text_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(image_chunks))\n",
    "\n",
    "img_data = base64.b64decode(image_chunks[2].content)\n",
    "img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Embedding Models  \n",
    "\n",
    "Once the text and images are divided into chunks, each chunk is converted into a numerical representation (embedding) that captures its meaning.  \n",
    "\n",
    "For text, we use OpenAI’s `text-embedding-3-large`, but other options exist, each with different trade-offs in on-premise vs online, accuracy, speed, and cost. The choice of model depends on the specific needs of the retrieval task.\n",
    "\n",
    "For images, we utilize `VLM2Vec`. Similar to text embeddings, various options exist for image embeddings, each with its own trade-offs. There will be one embedding model to convert images to vector representation, and another to convert the user query into the same representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = compute_openai_large_embedding_cost(text_chunks, verbose=True)\n",
    "\n",
    "\n",
    "# Check if both Azure environment variables exist\n",
    "azure_endpoint = os.getenv(\"AZURE_API_BASE\")\n",
    "azure_api_key = os.getenv(\"AZURE_API_KEY\")\n",
    "if azure_endpoint and azure_api_key:\n",
    "    text_embedding_model = OpenAITextEmbeddingsAzure()\n",
    "    print(\"Using AzureOpenAI client\")\n",
    "else:\n",
    "    text_embedding_model = OpenAITextEmbeddings()\n",
    "    print(\"Using OpenAI client\")\n",
    "\n",
    "    \n",
    "text_embeddings = text_embedding_model.get_embedding(\n",
    "    [chunk.content for chunk in text_chunks]\n",
    ")\n",
    "\n",
    "print(text_embeddings.shape)\n",
    "text_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_embeddings = []\n",
    "\n",
    "image_embedding_model = ImageEmbeddings()\n",
    "for chunk in tqdm(image_chunks):\n",
    "    image_embeddings.append(image_embedding_model.get_embedding(chunk.content))\n",
    "\n",
    "\n",
    "image_embeddings = np.array(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Also define the text embedding for the image-text embedding model\n",
    "image_text_embedding_model = ImageEmbeddingsForText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Vector Store and Retrieval  \n",
    "\n",
    "After embedding the chunks, they need to be stored for efficient retrieval. The choice of vector store depends on factors like accuracy, speed, and filtering options. In this exercise, we use `ChromaDB`.  \n",
    "\n",
    "The next step is retrieving the most relevant chunks based on a query. In this implementation, the retriever uses only embeddings (sparse search). However, in some cases, dense search methods like BM25 or hybrid approaches combining both sparse and dense search can be used for better accuracy when retrieving the text. Some retrieval strategies also use the metadata.\n",
    "\n",
    "Text and image embeddings are stored separately here, thus their similarities cannot be directly compared. As a consequence, the retrieval strategy implemented here is to take the `top_k` for each datatype.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store_text = ChromaDBVectorStore(text_vector_store_collection)\n",
    "vector_store_text.insert_chunks(text_chunks, text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store_image = ChromaDBVectorStore(image_vector_store_collection)\n",
    "vector_store_image.insert_chunks(image_chunks, image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = VectorStoreRetriever(\n",
    "    text_embedding_model,\n",
    "    vector_store_text,\n",
    "    image_text_embedding_model,\n",
    "    vector_store_image,\n",
    ")\n",
    "\n",
    "results = retriever.retrieve(test_question, top_k_text=5, top_k_image=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for result_l in results:\n",
    "    for result in result_l:\n",
    "        if result[\"chunk\"].data_type == DataType.TEXT:\n",
    "            print(result)\n",
    "        elif result[\"chunk\"].data_type == DataType.IMAGE:\n",
    "            print(f\"Chunk ID: {result['chunk_id']} | Score: {result['score']}\")\n",
    "            img_data = base64.b64decode(result[\"chunk\"].content)\n",
    "            img = Image.open(io.BytesIO(img_data))\n",
    "            plt.imshow(img)\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Generator  \n",
    "\n",
    "Once the LLM is set up, a specific prompt needs to be defined for the RAG system. This prompt must include the retrieved chunks as context. The prompt has to be adapted to each specific project.\n",
    "\n",
    "In addition to the basic prompt, we incorporate **prompt engineering** by asking the LLM to justify its answer. The model is also instructed to indicate which chunks were most relevant in forming its response, improving **interpretability**, and to provide the answer in **JSON format** for easier data management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_developer_prompt = \"\"\"You are a helpful assistant, and your task is to answer questions using relevant chunks and images. Please first think step-by-step by mentioning which chunks you used and then answer the question. Organize your output in a json formatted as dict{\"step_by_step_thinking\": Str(explanation), \"chunk_used\": List(integers), \"answer\": Str{answer}}. Your responses will be read by someone without specialized knowledge, so please have a definite and concise answer.\"\"\"\n",
    "print(default_developer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_rag_template = \"\"\"\n",
    "Here are the relevant CHUNKS:\n",
    "{context}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Here is the USER QUESTION:\n",
    "{query}\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "Please think step-by-step and generate your output in json:\n",
    "\"\"\"\n",
    "print(default_rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = Generator(llm, default_developer_prompt, default_rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, cost = generator.generate(\n",
    "    history=[],\n",
    "    query=test_question,\n",
    "    chunks=[\n",
    "        Chunk(\n",
    "            chunk_id=0,\n",
    "            data_type=DataType.IMAGE,\n",
    "            content=image_chunks[2].content,\n",
    "            metadata={},\n",
    "        ),\n",
    "        Chunk(\n",
    "            chunk_id=1,\n",
    "            data_type=DataType.TEXT,\n",
    "            content=text_chunks[0].content,\n",
    "            metadata={},\n",
    "        ),\n",
    "    ],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## RAG \"Tricks\"  \n",
    "\n",
    "There are several methods to improve the efficiency of a RAG pipeline, such as query contextualization, query reformulation, re-ranking, query expansion, etc. For the sake of time, none of those has been implemented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## RAG  \n",
    "\n",
    "Finally, the RAG pipeline is defined by integrating all the previously discussed components into a unified process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Only with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_without_images = DefaultRAG(\n",
    "    llm=llm,\n",
    "    text_embedding_model=text_embedding_model,\n",
    "    text_vector_store=vector_store_text,\n",
    "    generator=generator,\n",
    "    params={\"top_k_text\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, sources, cost = rag_without_images.execute(test_question, {}, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The chunks retrieved by the retriever:\n",
    "print(len(sources))\n",
    "print(sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## With text and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag = DefaultRAG(\n",
    "    llm=llm,\n",
    "    text_embedding_model=text_embedding_model,\n",
    "    text_vector_store=vector_store_text,\n",
    "    image_text_embedding_model=image_text_embedding_model,\n",
    "    image_vector_store=vector_store_image,\n",
    "    generator=generator,\n",
    "    params={\"top_k_text\": 5, \"top_k_image\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer, sources, cost = rag.execute(test_question, {}, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(json.dumps(answer, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The chunks retrieved by the retriever:\n",
    "print(len(sources))\n",
    "print(sources[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
